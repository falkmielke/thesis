\clearpage


\section{Abstract}
\label{sec:org608c530}
An essential part of the inverse dynamic workflow is the measurement of inertial properties, in particular mass, center of mass, and mass moment of inertia.
In the past, these have often been approximated by assigning average values to limb segments.
Recent studies have explored the potentially more accurate retrieval of inertials from CT data, which would work by associating voxel gray values of the CT image stack with densities.
However, CT images actually depict attenuation, which is a different attribute of matter and only roughly correlates with density.
Though this was known since the early days of CT, the temptation of easily retrieving inertials has recently led researchers to re-attempt linear approximation methods.

In this study, I document my own attempts in retrieving density values from CT images.
Though a conversion is possible by using a regression, I demonstrate that the resulting mass and inertia are highly erratic.
By performing a thorough sensitivity analysis, and by simulating common CT artifact, I can quantitatively evaluate what effect uncertainties in the regression have for the outcome measures.

The results are alarming: in the test case, mass is overestimated by more than a fifth, and error propagation indicates that this has an even worse effect on moment of inertia, which directly translates to errors in inertial dynamics.
However, there is no reason to expect alternative, conventional methods to be more accurate.
Thus, though CT images are not ideal for the purpose, they might be the best option available in most cases.
In conclusion, I emphasize the value of a sensitivity analysis, and indicate promising research progress of others which might lead to improving CT reconstructions for the purpose of estimating material characteristics.


\clearpage
\section{Introduction}
\label{sec:orgb80d559}
\subsection{Computed Tomography and Density}
\label{sec:org754f466}
Since the advent of x-ray imaging, people are intrigued by the ability to see the inner structure of objects and living creatures (such as the famous hand of Röntgen's wife, ``Über eine neue Art von Strahlen'', 1895).
This desire even increased by the development of computed tomography \citep{Beckmann2006,Hounsfield1973}, a set of techniques which enable the reconstruction and visualization of three-dimensional structural images.
The transmission images obtained via high energy electromagnetic radiation often serve to answer qualitative questions (e.g. whether a bone is fractured, whether there are porosities in asphalt drill cores).
Quantitative questions are obvious with regard to the shape of a scanned structure (e.g. the size of porosities, the length of a fracture, the shape of a bone).
However, researchers have been struggling with the quantitative extraction of material properties from Computed Tomography (CT) data.
The material property of primary interest is physical density; from a given density distribution, other relevant inertial properties can be calculated.


Reseachers suggested early on to relate the gray value of CT images to density \citep{Mull1984,Phelps1975}, yet it was immediately noted that the relation is nothing more than a correlation which only holds under specific circumstances.
Even these pioneer works acknowledge that there must be discrepancies between real and x-ray-derived densities, which are often associated with (i) the polychromatic character (source spectrum) of the used radiation, (ii) chemical composition (absorption spectrum), and (iii) scan artifacts.

These issues demand a detailed explanation, below.
Notwithstanding this list of known problems, people have repeatedly attempted to extract density and inertial propertes from CT scans  \citep[][, as well as the present study]{Phillips1997,DuPlessis2013,Durston2022}.
The purpose of this chapter is to explore whether or not (or under which circumstances) CT gray values can be used to estimate density distributions, and thereby mass and other derived inertial properties.
I will start by giving a brief intro to crucial aspects of CT scanning technology, before reviewing similar attempts by others, then introducing a simple experimental setup to measure dynamics of an excised piglet femur, and with that re-attempting the extraction of inertial properties.


\subsection{Emission and Absorption}
\label{sec:orge8c3674}
Visible light and x-ray radiation are, to a large degree, analogous.
The reason for this is that both are just electromagnetic radiation; they differ in wavelength and thereby energy.
To illustrate what is happening with x-ray light during a CT experiment, I will use the analogy of visible light.


On some military signal flashlights, one can set the light to be white (i.e. bulb spectrum, unfiltered) or filtered either red or green, sometimes a third color.
Light are photons, ``elementary particles which are the quantum of the electromagnetic field'' (\href{https://en.wikipedia.org/wiki/Photon}{wikipedia: photon}).
Filters let through some of them, while prohibiting others from passing.
The same principle holds for party lights or colored window sheets: a light source is placed behind one or more filter sheets which block most colors and let through certain others.
On the military flashlight, the ``white'' setting is general purpose, unfiltered, which will let pass the maximum number of photons you can expect from a given bulb.
``Lights'' (i.e. the item) are devices which constantly emit photons of a specific variety of wavelengths (colors); we call this variety the ``spectrum'' (see below).
Put simply, applying a red filter sheet will change the composition of the passing light by blocking all the ``non-red'' photons.
Because red photons pass the filter, the filtered spectrum will be predominantly red, and appear red to our visual perceptive system.

Most plants have evolved to be green; their cells hold organelles with the green pigment chlorophyll, which absorbs some non-green (e.g. red) wavelengths of light very well (Fig. \ref{fig:spectrum}, green curve).
The ensemble of non-absorbed photons appears green to us, which is why leaves generally appear green to our view.
But leaves only appear green if there is a green content in the incident light!
Green leaves will look red under red light.
Contrary to internet myth, the military use case of red flashlight would be an operation which requires as inconspicuous light as possible in a vegetated environment.
Red light will hardly reflect from gras or bush, and can therefore reduce the chance of being spotted at night by enemy surveillance.
If you want to avoid being seen while looking for something in a dark forest, use a red flashlight.


\begin{figure}[htbp]
\centering
\includegraphics[width=13cm]{./figures/spectrum.pdf}
\caption{\label{fig:spectrum}Spectra. The x-axis shows all wavelengths \(\lambda\) (\(nm\)) of relevance (in this case the range of visible light, for illustration). Related to wavelength and therefore viable axis label alternatives are frequency (\(Hz\)) and photon energy (\(keV\)). The y-axis shows photon occurrence probability, or normalized intensity \(I\), or the intensity difference \(\Delta I\) for the absorption spectrum. Black curve: emission spectrum of a white LED \citep{Tanabe2005}. Green curve: the approximate absorption spectrum of chlorophyll A \citep{Zscheile1934}. The chlorophyll will hardly get excited by the LED, yet the plant will look bright and green.}
\end{figure}


To characterize light which is emitted, reflected, or absorbed, one can plot a spectrum (Fig. \ref{fig:spectrum}).
Red LED's are an alternative to red filters: they immediately produce a spectrum which is biased towards low frequencies.
We can assess that red LEDs have a different \textbf{source spectrum or emission spectrum} than white LEDS or ``vintage'' (tungsten) light bulbs.
If the light from a white source is filtered (e.g. by a red sheet of thin plastic), the spectrum is altered.
A red LED viewed through a green-pass filter (e.g. a leaf) might appear to be ``off'', when really power is ``on'', because the majority of photons are absorbed by the filter.

In fact, light hitting \emph{any} object will change its spectral composition, depending on the material's \textbf{absorption, transmission, and reflection spectrum}.
This is also a filtering process: the outgoing light will depend on the incoming spectrum and the material properties of the object.
Plant leaves will almost entirely absorb a narrow band of red light.
Interactions of photons and matter crucially depend on the energy/frequency/wavelength of the photons, and the energetic/vibrational/resonant molecular properties of the matter.
A (differential) spectrum is a way to depict a wavelength dependence.


\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{./figures/Berger2018.png}
\caption{\label{fig:xray_emission}X-Ray emission spectrum. An x-ray source will emit photons with a variety of energies (``colors''). Bremsstrahlung will cover a wide range of photon energies, whereas discrete peaks are caused by specific emission processes in the target material. \citep[taken from ][, creative commons license]{Berger2018fig8}.}
\end{figure}

The situation is exactly analogous for x-rays \citep{Berger2018,Buzug2008}.
X-rays are also photons, just at a different wavelength (range \(10pm - 10nm\)).
X-ray sources are ``targets'', i.e. anodes made of certain metals (Tungsten, Molybdenum, \ldots{}), shot at with an electron beam which is evoked by applying voltage (e.g. \(60 kV\)) to the anode and a warmed-up kathode.
When hit by electrons, the target will emit x-ray photons of a specific composition of energies (spectrum, Fig. \ref{fig:xray_emission}).
The source emission is generally \textbf{polychromatic}, i.e. consisting of multiple colors/energies (just as the spectra in Figs. \ref{fig:spectrum} and \ref{fig:xray_emission}).
Most household CT scanners (in contrast to synchrotrons) have a polychromatic source.
On the other hand, x-ray detectors usually produce monochrome images, but are not monochromatic!
They integrate intensities over a wide range of wavelengths (in a specific way that could be measured as the sensitivity spectrum of the scintillator).
There is ongoing development on the frontier of ``spectral CT'' \citep{Liu2023}, yet resolution (spectral and spatial) are currently still below par (whereas price is not).


Once set on their path from within the x-ray source, x-ray photons interact stochastically with matter they encounter on their trajectory.
Much simplified, they are either absorbed or scattered (Rayleigh scattering, Compton scattering); \textbf{``attenuation''} is the term to describe that not all photons reach the detector.
Absorption is favourable, scattering is not, there can be secondary effects, and the probability of either of these interactions depends on (i) the wavelength (photon energy), (ii) the elementary composition of the material (absorption spectrum; characteristic bands), and (iii) the trajectory of the photon (thickness, angle).


The varying fraction of attenuated photons, measured from multiple incident angles for 3D view, is what enables the extraction of structural information (Lambert-Beer's Law).
If children place their hands inside a conical light beam, a shadow will be projected onto the wall.
Their hands attenuate the light.
Photos of the hand's attenuation pattern from all possible directions (i.e. rotating the hand) can in fact be used to reconstruct the 3D shape of the hand.

And attenuation is precisely the property which is thought to correlate with physical density: the higher the density, the higher the attenuation.
Or so it seems.
Yet think of a case in which the specific emission of the source does not match the absorption peaks of the material - remember the example of a white LED not exciting chlorophyll of a green leaf.
Or the opposite case, a substance with an absorption spectrum which is mostly congruent to the emission spectrum of the source.
Examples of substances problematic for x-ray are water and formol, because they absorb a broad range of photon energies within the x-ray range.


To summarize: both in the visual and x-ray range of electromagnetic radiation, emission and absorption are determined by stochastic interactions of elementary particles.
Spectra summarize ensemble properties of a given light source or material; differential spectra measure relative absorption.
The filtering properties of matter can be used to acquire images and reconstruct 3D structure, even in the absence of precise spectral information.


\subsection{Scan Artifacts}
\label{sec:org93ff288}
X-ray images do not always look as we would want them to look.
The unfavorable image features are commonly called ``artifacts''.
In the opinion of the provocative author, there is actually no such thing as CT scanning artifacts.
The term ``artifact'' implies that there is an unavoidable ``error'' in the measurement, yet instead it can be ascertained that correctly obtained x-ray images are highly accurate.
Any tomographic reconstruction just shows exactly what is measured, convoluted with ideally negligible reconstruction algorithm characteristics.


Acknowledged, some aspects of the measurement might be unfavorable to the observer, because they deviate from the image which that observer expects, based on their personal experience of the real world.
For example, we speak of a ``beam hardening artifact'' if absorption in the superficial layers of an objects alter the spectrum of the beam on its trajectory, which will affect the virtual representation of the deeper regions \citep{VanGompel2011}.
An observer will know that a cylinder is homogeneous, and reject the image which shows a radial density gradient.
However, that gradient is in fact a normal manifestation of the actual physical process (the stochastic interaction of electromagnetic radiation and matter, see above).
Curiously, beam hardening can be minimized by pre-hardening the beam with the use of metal filter plates.

Similarly, ring artifacts stem from sensitivity variations on the image detector, which are technically inevitable (due to constraints of the physical detection process), but can be rectified reliably \citep{Sijbers2004}.
Partial volume effects are caused by finite scan resolution and voxel volume.
Streak artifacts are caused by limited dynamic range and ``photon starvation'' (i.e. beam hardening, again).
All these could be considered properties of the scan, rather than interpreting them as an annoyance.

Another group of so-called artifacts might stem from the choice and limitations of the reconstruction algorithm.
There are iterative/algebraic and analytical reconstruction methods \citep{Gilbert1972,Andersen1984,Feldkamp1984,Geyer2015,Hansen2021}, all of which have their specific limitations.
These algorithms are constantly refined and improved by capable scientists, and specific algorithm variants can already overcome specific scan limitations \citep[e.g.][]{Six2019,Frenkel2022}.
In the future, the question of reconstruction artifacts will rather be one of algorithm choice.
The most common reconstruction algorithm, used in a vast majority of the CT service facilities with cone-beam setups, is ``filtered backprojection'' (FBP, or Feldkamp/Davis/Kress = FDK algorithm).


To summarize: scan artifacts, if we want to use that label, are perfectly normal.
Some are to a certain degree avoidable, others show intrinsic features of the technical and computational tomography procedure.
Artifacts are interpreted as ``something is not as it is supposed to be'', despite attenuation-based images being close to technical perfection.
Artifacts are in conflict with our psychological image of the real world: metal beads do not look like ``stars'' (radial light rays) to us in the real world, so we reject the image.
I suspect that the major reason we still perceive artifacts as problematic is that we actually think of matter and physical objects as a distribution of density, i.e. a mass distribution, whereas x-ray scanning really yields a distribution of x-ray attenuation.

On the other hand, if we assume that density cannot be recovered from attenuation images, the only appropriate way to measure an exact mass distribution of an object would be to slice it into little voxel cubes and weigh each of them.
In that sense, computational tomography is certainly a time-saving alternative.


Yet it must be kept in mind that x-ray images do not quantify density, they quantify attenuation, most often lumped over a spectrum.


\subsection{Density Approximations: Two Case Studies}
\label{sec:orgf339bf1}
The fact that two physical properties (attenuation and density) are fundamentally different things does not imply that researchers cannot use one to measure the other.
Scientists have repeatedly suggested and attempted to convert CT scan gray values to approximate density \citep{DuPlessis2013,Durston2022}.


For example, \citet{DuPlessis2013} acknowledge difficulties of accuracy and repeatability in extracting density from CT data.

They then average gray values of putatively homogeneous blocks of polymer plastic material, apply linear regression which, as they point out themselves, does not appropriately cover two of the measured data points.
The latter problem is attributed to differences in chemical composition (without discussing the known composition of the objects).
The authors then argue that chemical composition can be assessed by performing a dual energy CT scan (DECT), i.e. scanning at two different tube voltages and taking the ratio of gray values.
Note that this version of DECT is not really ``dual'' energy: the spectrum of energies elicited in a \(60\ kV\) scan is included in the \(230\ kV\) scan; the photons below \(60\ kV\) might even contribute the majority of light in the high energy scan.
A better differential would have been achieved by incorporating metal filters in the \(230\ kV\) scan.
Nevertheless, comparing the calibration line and the DECT ratio results, the putative outliers do not stand out more than other regression elements (especially PTFE puts the original regression in doubt: it is perfectly fit by the calibration regression, is a modest outlier on the DECT ratio, despite a special chemical composition containing fluorine).

For a toy pig of unknown plastic material, the authors get higher than actuall mass estimates; they identify chlorine and calcium content as the cause.
On another unknown sample which is assessed to be ``similar enough'' in chemical composition, they retrieve an accurate prediction.
The authors do not quantify or report measurement uncertainty which manifests in gray value distributions, suboptimal regression goodness of fit and potentially high regression sensitivity (PTFE), or other sources of variability \citep{Macaulay2017}.
Finally, they destroyed a toy pig for the study, which cannot be excused.

In a nutshell, \citet{DuPlessis2013} attempt density prediction in a particular use case (homogeneous, chemically identical objects) and suggest a DECT ratio to assess chemical composition.


\citet{Durston2022} attempt a huge leap from there and measure inertial properties of frozen cadaver parts, both conventionally and digitally.
Emphasis: they use whole birds, and their considerable amount of work must be appreciated!

As the authors above, Durston et al. acknowledge the critical assumption of a linear relation between attenuation and density, and consequentially use a simple linear relation as a conversion from CT scan in Hounsfield Units to actual density.
They supplement the calibration regressions, which show systematic errors at close look (the regression line lies tilted compared to the majority of relevant calibration objects; it looks biased by the ``air'' sample point yet that is a valid and important reference point; the authors do not discuss this).
Still, that the linear fit works at all is surprising, given that this study uses tissue phantoms provided for medical CT, all of which will putatively have a slightly different chemical composition (linearity assumption violated).

To validate their results from CT density estimates, the authors apply two approaches.
The first is a comparison of virtual and physical dissection, with regard to segment mass measurements.
The second is a trifilar pendulum ``ground truth'' for one axis of the mass moment of inertia.
There is also a validation of the pendulum method, by applying it to manufactured nylon blocks, yet quantitative error estimates remain vague.

Overall, results of the \citet{Durston2022} study remain superficial considering the author's valuable efforts on this project.
They juxtapose pie charts of segment masses to verify mass distribution, which is far too inaccurate.
They compare the virtually and physically derived dorsoventral moment of inertia, and present what must be a clear mismatch, considering the lack of meaningful error margins.
They discuss the influence of partial volume effects on density (which, agreeably, could be a problem with feathers), however that is irrelevant for mass and moment of inertia because volume and density both change if a voxel contains air and tissue.
They present ``moment of inertia distributions'' in various ways, i.e. the contribution of each voxel to a COM-centered \(I\) for an entire articulated skeletal system, which is of little practical relevance (which they confirm themselves, by comparing extended and retracted wing configuration).
It is a good reproducibility control that their data confirms previous findings of segment masses of some bird species; yet whether the moment of inertia is of any relevance for flying birds of prey who are presumably operating in an air drag regime (light bones and feathers, large wingspans) remains questionable.



This critique of the studies above is harsh, and I apologize for pointing a finger.
Many other studies have lumped the whole scan volume into a single density material, whereas the presented examples explore potential improvements.
Yet there is a unifying feature and a reason for the highlighted flaws in theses and other studies: they are \emph{output driven}, and fall short of discussing the mechanistics of CT imaging.
As shown in these examples, practitioners often simply assume that structural CT data represents physical density, instead of failing to falsify this claim.
The results are studies which yield approximate density distributions, yet fail to quantify the inaccuracy and uncertainty of their quantitative data.


The author of this thesis is no exception.
As the authors of the studies reviewed here, I lacked insight and was driven by good hope when starting the work on the present project.
In consequence, this whole study is one that proves its own irrelevance by falsifying the hypothesis that CT gray values can be converted to physical density.
\emph{Mea culpa.}
By documenting and discussing these failed attempts in this chapter, I hope to spare future researchers from the same fault.


\subsection{Reductionist Approach}
\label{sec:org53ebda5}
This study follows up on the idea of using a calibrated CT scan \citep{DuPlessis2013}, and adds a biologically relevant test object as well as an evaluation of the error margins, which is crucial for practical use.
Calibration is attempted with cheap, leftover plastic pieces of known material and density, as well as commercial bone mineral calibration phantoms.
We chose a dissected porcine femur as the object of interest of which we seek to estimate the density distribution, and thereby total mass, center of mass, and mass moment of inertia (Fig. \ref{fig:femur_scan}).
We also attempted to validate the method on dissected animal specimens \citep[as][]{Durston2022}.

\begin{figure}[htbp]
\centering
\includegraphics[width=18cm]{./figures/femur_scan.jpg}
\caption{\label{fig:femur_scan}\textbf{Experimental Setup.} A piglet femur was excised an marked with metal bead markers. Together with various calibration objects, the sample was fixed (``mounted'') in a PET bottle to fit into the cylindrical scan volume of \href{https://www.uantwerpen.be/en/research-groups/vision-lab/x-ray-imaging-resear/flexct}{the FleXCT scanner of the University of Antwerp}. The image on the left shows the sample mount, magnified on the inset. On the right is one CT projection of the sample, with the femur with markers clearly visible on top.}
\end{figure}


The hypothesis that CT images are a valid approximation of physical density is already falsified by the theoretical considerations above.
The following relevant questions remain:
\begin{itemize}
\item Are the cheap plastic parts appropriate calibration options for organic tissue?
\item How far off the true value is the calculated mass moment of inertia, i.e. what is the measurement error?
\item How much do known artifacts contribute to the measurement error?
\end{itemize}


It should be pointed out that preliminary (naïve) results of this study were first presented at the \href{http://mielke-bio.info/falk/posts/26.seb2021}{conference of the Society of Experimental Biology (SEB) in 2021}, one year prior to the \citet{Durston2022} study.




\section{Materials and Methods}
\label{sec:org1185d71}
\subsection{The Flying Femur}
\label{sec:org45b0e81}

As discussed in the previous chapter, the goal of inverse dynamics is to calculate the segment-wise balance of wrenches, thereby elucidating which forces and moments each joint has to handle in a motor task.
The unit of calculation is therefore a segment.
In a reductionist approach, the purpose of this experiment is to perform all procedures and calculations on one extracted segment.

For this purpose, a femur was excised from a piglet specimen which had been used previously in XROMM\footnote{X-ray Reconstruction Of Moving Morphology, Brainerd et al., 2010} experiments (see below). \nocite{Brainerd2010}
The animal was euthanized after succesful completion of the experiment.
Procedures have been approved \emph{a priori} by the Ethical Committee of Animal Experimentation, University of Antwerp, Belgium (approval number 2017–25).
The femur was extracted by carefully disarticulating the parts of the right hindlimb of the piglet, isolating the femur, and removing surrounding soft tissue.

\begin{figure}[htbp]
\centering
\includegraphics[width=18cm]{./figures/flying_femur_fotos.png}
\caption{\label{fig:flying_femur_fotos}The ``flying femur'' experiment: an excised piglet femur used in XROMM experiments as an allegory of reductionism. Left: the femur oscillating on a pendulum. Right: installation with a motor capable of moving the femur in a realistic way.}
\end{figure}


The extracted bone underwent the full XROMM procedure (Fig. \ref{fig:flying_femur_fotos}).
Metal bead markers, \(0.5 mm\) standard soldering balls made out of a lead-tin alloy (\(Sn_{63}Pb_{37}\)), were glued to the extracted bone to simulate the typical XROMM necessity of marked bones and facilitate video digitization.
Biplanar x-ray video recordings were performed on the \href{https://www.uantwerpen.be/3d2ymox}{University of Antwerp's 3D²YMOX system} \citep{Nguyen2021,Sanctorum2020}, with two experimental settings.
In one setting, the femur hung on a long, thin wire pendulum (appearing on camera to be ``flying'').
In a second setting, the bone was rotated to mimic in vivo motion by a motor to which it was attached with an ingenious, radiotranslucent, custom-made plastic clamp.

After the XROMM experiment, the femur was subjected to a calibrated micro-CT scan at a local micro-CT facility.
Plastic parts for calibration were donated by the mechanical workshop of the university (who thought it was worthless waste), covering a range of physical density from polypropylene (PP, \(916 \frac{kg}{m^3}\)) to Polytetrafluoroethylene (PP, \(2210 \frac{kg}{m^3}\)).

In addition to the plastic debris, two professional calibration phantoms (Bruker, USA) were included in the scan.
Those are hydroxyapathite cylinders, immersed in water over night, with a diameter of about half a centimeter and a length of a centimeter.
They calibrate bone \emph{mineral} density of \(0.25\ \frac{kg}{m^3}\) and \(0.75\ \frac{kg}{m^3}\), and they have a \emph{physical} density of \(1254\ \frac{kg}{m^3}\) and \(1485\ \frac{kg}{m^3}\).
The latter was measured by dividing the weight (measured with a fine scale) and the volume (from CT scan), after the author had found out that those are different quantities.
Air and tap water volumes in the scan have known densities and complete the calibration series.


Expected outcomes of the ``flying femur'' experiment are twofold.
Firstly, this provides a test case as a reference for actual XROMM, just with simpler calculations, but including an ``order of magnitude'' estimate of the moments and forces required to move the bone (motor experiment; not shown).
Secondly, the validity of the ``calibrated CT'' method for extracting inertial properties is to be evaluated.


\subsection{Piglet Data}
\label{sec:orgb7cd81d}
Originally, the femur was part of an animal, and thus also part of a bigger whole set of experiments.
That project involved newborn piglets reared temporarily at the veterinary facilities of the University of Antwerp, and subjected to XROMM recording sessions.
Experiments were approved by the Ethical Committee of Animal Experimentation, University of Antwerp, Belgium (approval number 2017–25) and performed in June and July 2019.

The part of these experiment relevant for this chapter are the CT scans.
Those were not calibrated with the full array of plastic debris; they contained only air, water, and the bone mineral density phantoms.
One of the piglet scans was chosen for the validation of the density acquisition method by comparing known segment- and total weight to the outcome of the CT method.
For this purpose, the animal was dissected both physically and virtually (segmentation, Fig. \ref{fig:piglet_segmentation}) and the results compared.
Segments were weighted individually on a precision scale.
Virtual segments were processed with the density regression procedure (see below) to compute segment-wise mass estimates.

\begin{figure}[htbp]
\centering
\includegraphics[width=12cm]{./figures/virtual_dissection.png}
\caption{\label{fig:piglet_segmentation}Virtual dissection (segmentation) of a piglet CT scan. Example virtual slice.}
\end{figure}


\subsection{Scan Parameters}
\label{sec:org7ee5220}

\begin{table}[htbp]
\caption{\label{tab:ct_settings}CT scan settings for the calibrated femur scan.}
\centering
\begin{tabular}{|l|c|}
\hline
voltage & \(150 kV\)\\[0pt]
power & \(55 W\)\\[0pt]
current & \(\approx 360 mA\)\\[0pt]
filter & \(1.0 mm\ Al\)\\[0pt]
detector field & \(1920\times 1896 px\)\\[0pt]
pixel size & \(0.15 mm\)\\[0pt]
exposure time & \(50 ms\)\\[0pt]
averages & \(3\)\\[0pt]
projections & \(2879\)\\[0pt]
source-detector & \(800 mm\)\\[0pt]
source-object & \(267 mm\)\\[0pt]
binning & \(none\)\\[0pt]
resolution & \(50 \mu m\)\\[0pt]
scan duration & \(10 min\)\\[0pt]
reco value range & \([-0.2, 1.0]\)\\[0pt]
\hline
\end{tabular}
\end{table}

The scan was performed on a customized Tescan Unitom XL, with appropriate settings (Tab. \ref{tab:ct_settings}).
The \(1.0 mm\) Aluminium filter plate and the relatively high voltage were chosen to reduce beam hardening and metal bead artifacts (see below).
Three averages indicate that every projection is the average of three scan images from the same angle, which is a common trick to reduce pixel noise.
Scan geometry was set to give sufficient resolution at reasonable scan duration.
The reconstruction value range was chosen to cover the entire histogram, excluding the metal beads, which gives best dynamic range on the plastic and organic tissue.


The piglet scans for the second experiment were performed at a different facility and with slightly different settings.
Piglets were scanned in frozen state and in bags of two individuals at a time, to save time and cost.
Air, water, and the bone mineral density phantoms were available for calibration in these scans.
Resolution was lower, but exposure settings were comparable to the ones on the femur experiment.


\subsection{Inertial Properties}
\label{sec:orgcb7dc22}

All code used on this project, including python implementations of the mathematical formula's below, can be \href{https://git.sr.ht/\~falk/flying\_femur}{found online} (\nolinkurl{https://git.sr.ht/~falk/flying_femur} and \nolinkurl{http://mielke-bio.info/falk/posts/23.ct_density}).

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{./figures/inertial_properties.pdf}
\caption{\label{fig:inertials}Calculation of inertial properties of a limb segment (femur) from CT scans. CT scans are 3D images, consisting of voxels (cubes) each of which is associated with a gray value. If these could be associated with physical density, it would be possible to calculate the mass, center of mass (COM) and mass moment of inertia. In the calculation procedure, voxels are treated as little mass elements \(m_{i}\), which are positioned at a given vector position \(r_{i}\) from an arbitrary CT origin.}
\end{figure}

\subsubsection{CT Segmentation}
\label{sec:orga28902e}
Of course, if e.g. the mass of a bone is to be calculated, one intends to sum up the mass of only the bone, excluding the mass of the surrounding air or support material.
A preliminary step to calculate inertial properties from CT scans is the segmentation of the scan.
Segmentation is the separation of the ``relevant'' and ``irrelevant'' sub-volumes within the image stack, in this case the bone and the surrounding air or background.
More generally, limb segments which are treated as a unit have to be labeled in dedicated software (e.g. \href{https://www.slicer.org}{3D Slicer}), typically with a kind of ``color brush'' or ``magic wand'' tool; thresholding of grey values, boolean operations, and other tricks may simplify the segmentation procedure in certain situations.
The outcome is a 3D boolean mask which can be used to distinguish the voxels in the scan which are associated with the bone of interest.

This step might seem trivial, time consuming, yet necessary, and indeed it is all of that.


\subsubsection{Mass}
\label{sec:orgc67bdd5}
The mass of a volume element \(m_{i}\) of a rigid body is calculated as the density \(\rho_{i}\) of that element, multiplied with its volume \(V_{i}\) (Fig. \ref{fig:inertials}).
Summing up all volume elements will give the total mass of the object:
\begin{equation}\label{eqn:mass}
\sum_{i} m_{i} = \sum\limits_{i \in V} \rho_{i} V_{i}
\end{equation}

The crucial part here is to get \(\rho_{i}\), the density per voxel, which we attempt via a regression (see below).


\subsubsection{Center of Mass}
\label{sec:orgb72de19}
The center of mass is the mass-weighted average position vector of all volume elements of a rigid body (Fig. \ref{fig:inertials}).
For each volume element \(i\), mass \(m_{i}\) is a scalar, the position vector \(r_{i}\) is a three-by-one vector, and their product is summed up for all voxels of the segmented bone.
The result is normalized by dividing the total mass.
The outcome is the three-by-one position vector of the center of mass (COM, shorthand for \(r_{COM}\)).
\begin{equation}\label{eqn:com}
 COM = \frac{1}{\sum_i m_{i}} \sum\limits_{i \in V} m_{i} r_{i}
\end{equation}


\subsubsection{Mass Moment of Inertia}
\label{sec:org04c9f63}
As stated before, mass moment of inertia is the resistance of an object to angular acceleration.
It can be calculated for any rigid body (see Fig. \ref{fig:inertials}) via an integral formula over all volume elements \citep{WikipediaMOI}:
\begin{equation}\label{eqn:mmoi}
 I = \sum\limits_{i \in V} m_{i} \left( \lVert r_{i} \rVert ^2 E_3 - r_{i} \otimes r_{i} \right)
\end{equation}

\ldots{} with the rigid body's volume \(V\) split up into voxels \(i\) which have mass \(m_i\) and position vector \(r_{i}\); \(E_{3}\) is the \(3\times 3\) identity matrix and \(\otimes\) the outer product.

Initially, this is always calculated with respect to the COM as reference point, the algorithm usually includes subtracting the COM from the position vectors.
Otherwise, the reference point can be changed by the generalized parallel axis / Steiner's theorem \citep[][p. 245]{Lynch2017}.
\begin{equation}\label{eqn:steiner}
 I_{p} = I_{0} + m \cdot \left( s^{T} s E_{3} - s s^{T} \right)
\end{equation}

\ldots{} with \(s\) being the shift vector connecting the original and target points.


It is difficult to get an intuition about Mass Moment of Inertia, but classroom demos can illustrate that this measure depends exclusively on the geometric distribution of mass of an object \citep{Lewin801L19,LewinMOI}.
This is also apparent from equation \eqref{eqn:mmoi} above.
For symmetrical objects, it is independent of the total mass.
Under controlled experimental conditions, a homogeneous aluminium cylinder will roll down a slope in exactly the same time as a homogeneous lead cylinder of identical form.
This has implications for interpreting the ``beam hardening simulation'' results below.


\subsection{Density Regression}
\label{sec:orgd2f0cae}
The critical step in the procedures above is the relation of gray values \(\gamma\) and physical densities \(\rho\).
This is the search for an (idealized) function \(\gamma = f(\rho )\), which can predict the CT gray value for any given density.
The inverse, \(\rho = f^{-1}(\gamma )\), can then be used to assign densities to gray values from the scan.
Several regression functions were attempted, based on a guessed relation of the observed gray values of the calibration objects.

The regression was performed in Python, namely \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html}{using the \texttt{scipy.optimize.minimize} function} to minimize the Euclidean difference between observed and fit values with the Nelder-Mead algorithm, tolerance set to \(10^{-16}\) \citep{Gao2012}.
Convergence was supported by setting meaningful parameter start values close to the expected outcome (Tab. \ref{tab:regressions}).

\begin{table}[htbp]
\caption{\label{tab:regressions}Density Regression Functions and regression start values.}
\centering
\begin{tabular}{|l|l|l|}
\hline
 & function & start values\\[0pt]
\hline
\hline
linear & \(a + b \cdot x\) & \(a=0.175, b=10^{-4}\)\\[0pt]
exponential & \(a+b\cdot e^{c\cdot x}\) & \(a=0.12, b=0.05, c=0.0015\)\\[0pt]
\hline
\hline
\end{tabular}
\end{table}

Ideally, all measured points would lie on either of these functions.
Yet that turns out not to be the case in the actual data.
Subsets of the calibration objects were selected for the regression to be plausible: one group were air and the plastic parts, which approximate a linear relation; the other group were air, water, and the two bone mineral density phantoms, which seem to follow an exponential relation.


\subsection{Scan Artifacts}
\label{sec:org52dbaff}
To estimate the effect of beam hardening artifacts, one of the scanned calibration objects was selected by segmentation and virtually modified.
I selected PTFE, because it has the highest attenuation in the data set and is therefore most prone to suffering from beam hardening.
The PTFE cylinder has almost ideal cylindrical shape, and was well aligned with the scan rotation axis.
The object was segmented and cropped out of the whole scan so that the long axis of the cylinder aligns with the center of the cropped volume.


Beam hardening manifests in a radial attenuation gradient in objects: superficial layers which are reached first by x-ray beams seem to attenuate more than internal volume elements.
The attenuation gradient produces a cup-like profile in the gray value images, which is why beam hardening of cylinders is called ``cupping''.
To quantify the amount of cupping, the cylinder was transformed to cylinder coordinates (scikit image/transform/warp polar, \nolinkurl{https://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.warp_polar}), and then flattened by averaging along its vertical (long) axis.

\begin{equation}\label{eqn:expfit}
 I = a+b\cdot e^{k\cdot r}
\end{equation}

By using an exponential regression \eqref{eqn:expfit} to the exponential part of that cylinder profile, one can extract a parameter \(k\) which quantifies the amount of cupping (\(k\) as in the Dutch word ``kopje'').
With that \(k\) known, one can rectify the gray values of the scan by applying a correction factor which would push up the exponential line to a flat constant (obviously not applicable for irregularly shaped objects).

Similarly, one can multiply the intensity values of each voxel in the original volume with an exponential of its distance to the center point.
Depending on the chosen value for \(k\) in that exponential, one can virtually manipulate the amount of beam hardening (Fig. \ref{fig:cupping_methods}).


\begin{figure}[htbp]
\centering
\includegraphics[width=18cm]{./figures/cupping_methods.pdf}
\caption{\label{fig:cupping_methods}Beam hardening artifact simulation. The top view (A) and side view (B) of a PTFE cylinder with beam hardening strength \(k\) virtually set to \(k=0.5\). (C) Line profile of scan gray values along the blue line indicated in panel A shows the typical ``cupping''. (D) cylinder coordinate transformation of the slice marked by a blue line in panel B gives the gray values along the radial lines \(r\) at angles \(\phi\) indicated in panel A. (E) Top and side view for different simulated beam hardening strengths \(k\).}
\end{figure}


The focus of this virtual experiment is mass moment of inertia.
One might argue that, though the value of the tensor \(I\) depends on the mass, its principal axis orientation only depends on the mass distribution.
Given that we expect a mismatch of the calculated total mass and the actual measured total mass of the test object, an appealing strategy might be to scale the voxel-wise masses so that total masses match.
We call this strategy ``scaling'', and it is optionally compared to the ``unscaled'' gray values of the simulated PTFE cylinders.



\subsection{Error Propagation}
\label{sec:orgae14f30}
What magnitude do miscalculations of the moment of inertia have, and how much of a problem is that for calculations of joint moment?
To evaluate this, one has to consider error propagation with the balance equations \eqref{eqn:moment_error}.
The variable \(I\) only enters the balance equations through the dynamic wrench, \(M = I \ddot\theta\).
This function is linear, so error propagation is quite simple\footnote{We should not assume that the error in measuring \(\ddot \theta\) is zero, but will here focus only on the \(I\) component.} \citep{Hughes2010,Normann2016}:
\begin{equation}\label{eqn:moment_error}
\Delta M = \sqrt{\left(\frac{\partial M}{\partial I}\Delta I\right)^2} = \sqrt{\left(\Delta I \ddot\theta\right)^2}
\end{equation}

Sensitivity is linear at this level: any percentage of measurement inaccuracy in \(I\) will directly propagate to the joint moment calculation.


Yet the situation is more involved: calculating \(I\) via equation \eqref{eqn:mmoi} is itself subject to errors, by inaccuracy in the mass itself (regression), and by unceretainty in the COM position and therefore the position vectors \(r_{i}\) of each mass element.
\[ \Delta I = \left. \Delta I \right|_{m_i} + \left. \Delta I \right|_{r_i} = \sqrt{ \sum_{i} \left[ \left(\frac{\partial I}{\partial m_{i}} \Delta m_{i} \right)^2 + \left(\frac{\partial I}{\partial r_{i}} \Delta r_{i}\right)^2 \right] }\]


The direct dependence on mass is visible in equation \eqref{eqn:mmoi}, uncertainty of the mass elements contribute linearly but are added up; errors in the mass of further away mass elements contribute more to the total error of the moment of inertia.
\[ \Delta I \mid_{m_i} = \sqrt{\sum\limits_{i}\left(\left( \lVert r_{i} \rVert ^2 E_3 - r_{i} \otimes r_{i} \right)\right)^2 \cdot \Delta m_{i} }\]


The partial derivative with respect to \(r_{i}\) must be added, because \(I\) is taken relative to the COM (hence, \(\Delta r_i = \Delta COM\)).
We know that the choice of reference point for moment of inertia can be adjusted by using generalized Steiner, and so the error in the COM propagates as a Steiner component, equation \eqref{eqn:steiner}, with \(s=\Delta COM\).
Yet \(\Delta COM\) also depends non-trivially on the relative mass of volume elements, equation \eqref{eqn:com}.
\[ \Delta COM = \sqrt{ \frac{1}{\sum_i m_{i}} \sum\limits_{i} \left( r_{i} \cdot \Delta m_{i} \right)^2  + \left( \frac{ \sum\limits_{i} r_{i} \cdot m_{i} }{\Delta \sum_i m_{i} }\right)^{2}}\]


With this last equation we have lined out how the target parameter, \(I\), ultimately depends on mass elements \(m_{i}\) and their position in different ways.


In practical terms, the expected error can be readily computed by iteratively changing the variables of interest by a percent, to infer how ``a percent change'' in a parameter effects the result of the calculation.
This is a numerical \textbf{``sensitivity analysis''}.
From the analytical considerations above, we can see that the variables to change in the numeric computation are the regression parameters (\(\pm 5\%\) of their value), the COM position \(\pm 5\%\) of femur length, \(0.25 mm\) in each direction), and the total mass (\(\pm 5\%\), which are \(0.26 g\)).


\subsection{Error Estimation}
\label{sec:org5e8e546}
Calculating the effect of an error is only one part of sensitivity analysis.
The second, equally relevant part is an estimate of how uncertain parameter estimates actually are.
A great way to measure this are probabilistic statistics (Ch. \ref{cpt:statistics}).
Besides yielding outcomes for the intercept and slope, they also allow to estimate the uncertainty of these parameters by sampling a model residual error.


To estimate the uncertainty in the density regression, I repeated the procedure in a probabilistic modeling framework \citep[PyMC, Version 5.6,][]{Salvatier2016}.
The linear regression function was almost unaltered:
\[\gamma = a+b\cdot\rho + \varepsilon_{j}\]


In this framework, the residual \(\varepsilon\) is calculated, measuring how much residual difference remains after optimization of the regression parameters.
In a linear regression, \(\varepsilon\) corresponds to an uncertainty in slope.
Beyond the standard model residual, I introduced \(\varepsilon_{j}\), with index \(j\) referring to the materials used for calibration.
Each material thus receives its own residual, all of which are drawn from a higher level normal \(\varepsilon\) (hierarchical model design).
This helps to account for systematic increase of the gray value uncertainty with increasing density (``widening'' of the distribution), as it was also observed in \citet{DuPlessis2013}.

Hence, this regression serves three purposes:
\begin{itemize}
\item confirm results of the conventional least squares regression
\item provide a measure for the intercept uncertainty (\(\varepsilon\))
\item identify potential slope uncertainties (\(\varepsilon_{j}\))
\end{itemize}


All parameters were approximated by Normal distributions, except for the \(\varepsilon_{j}\) (HalfNormal, i.e. bound to be \(>0\)).
Results from the conventional regression and wide enough standard deviations were used as priors to initialize the sampling.
The posterior distributions were calculated from the last \(4\cdot 2^{12}\) samples (after sufficient tuning of the NUTS sampler).
Gelman-Rubin statistics and trace plots indicated convergence and well-behaved sampling.


One important distinction from least squares regression is that it does not require averaging of the observed gray values.
Hence, the regression took a large number of voxels into account, capturing the variability in gray values of the homogeneous polymer materials.
A subset of \(10000\) randomly selected voxels per material were added to the data array.
Subsetting is necessary because the number of observations adds computational load for the MCMC sampler.


\section{Results}
\label{sec:orgd338191}
\subsection{Density Calibration Regression}
\label{sec:org880496a}
\begin{figure}[htbp]
\centering
\includegraphics[width=16cm]{./figures/regression.pdf}
\caption{\label{fig:density_calibration}Density calibration of a CT scan. The polymer materials show a linear relation between density \(\rho\) and CT gray value \(\gamma\) (exception: PVC, excluded from regression), whereas the positions of BMD phantoms (ld: low density; hd: high density) implies an exponential relation. Violins illustrate the distribution of gray values, which appears due to stochastic attenuation, yet each object in the scan is associated with exactly one true (average) density.}
\end{figure}

To establish a transform function which can convert CT gray values \(\gamma\) to physical density \(\rho\) and back, we compared different regression functions which fit the observed gray values of different polymer materials of known densities (Fig. \ref{fig:density_calibration}).

The random plastic parts obtained from the workshop have known densities, and most of them show a linear relation when plotting \(\gamma\) against \(\rho\).
The exception is PVC, which was therefore excluded from the regression (whether it was truly PVC, or a mislabeled different polymer, cannot be assessed retrospectively).
Bone mineral density (BMD) phantoms and water do not fall on that line.
Their gray values imply an exponential relation of \(\gamma\) and \(\rho\).
Both regressions were performed by minimizing least square difference of observations and the target function.
Both the linear and exponential function can be inverted.
The femur of unknown density distribution lies right between the two regression lines, but spans a large range of gray values and densities because of its heterogeneous composition (marrow, cavities, muscle tissue, calcified bone, metal markers and glue).

With the regression outcome, it is technically possible to convert any observed or simulated voxel gray value to putative density values, which applies for example to the femur.


\subsection{Issue 0: Mass Mismatch and Total Mass Normalization}
\label{sec:org183d29d}

To confirm the outcome of the regressions above, we compared the actual mass of the femur (which is \(5.271 g\)) to the outcome of the digital approximation.

As seen in the gray value distributions (Fig. \ref{fig:density_calibration}), the majority of femur voxels have a gray value which falls right in between the two regression lines.
This propagates to the calculated outcomes.
With the linear regression, the femur mass is calculated to be \(6.38 g\) (\(+21 \%\)).
In contrast, the exponential regression yields a lower than actual mass (\(4.96 g\), \(-29 \%\)).
Though it appears that the correct regression would lie somewhere between the exponential and linear attempt, there is no data, nor theoretical justification supporting an intermediate hypothesis.
Neither is there any reason to think that all biological tissues generally fall in between the presented cases: for other samples, composition might be different.

We conclude that, at least for a femur, the regression method is not reliable.


To exclude that problems arose from the special composition of this isolated bone, we applied a similar regression method to more complex data.

In a second experiment, a piglet body was CT scanned, virtually segmented, and analogously physically dissected to see whether the total calculated masses of the virtual segments match their actual counterparts.
The corresponding scan was performed at a different facility and long before the density calibration experiments, and thus the only available calibration points were air, water, and two density calibration phantoms.

Although an exponential regression was used (under-estimation in case of the femur), this virtual density conversion leads to a considerable over-estimation of the mass of most of the segments, and the specimen total.

The contradicting results highlights how erratic the digital density approximation is.



\begin{table}[htbp]
\caption{\label{tab:pigletdissection}Segment mass verification on an actual piglet specimen. On the lighter segments, measurement accuracy and rounding are limiting. Note that the total mass at euthanasia was measured \(1.125 kg\), a slight loss is expected due to condensation and preservation.}
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{segment} & \textbf{CT segmentation} & \textbf{dissection} & \textbf{difference}\\[0pt]
\hline
\hline
\textbf{masses \([kg]\)} &  &  & \\[0pt]
head & 0.247 & 0.230 & +7\%\\[0pt]
torso+trunk & 0.666 & 0.580 & +15\%\\[0pt]
LF humerus & 0.033 & 0.027 & +22\%\\[0pt]
LF radioulna & 0.016 & 0.014 & +14\%\\[0pt]
LF metacarpal & 0.009 & 0.006 & +50\%\\[0pt]
LF hoof & 0.006 & 0.005 & +20\%\\[0pt]
LH femur & 0.052 & 0.050 & +4\%\\[0pt]
LH tibiafibula & 0.027 & 0.020 & +35\%\\[0pt]
LH metatarsal & 0.012 & 0.010 & +20\%\\[0pt]
LH hoof & 0.004 & 0.004 & +0\%\\[0pt]
RF humerus & 0.031 & 0.031 & +0\%\\[0pt]
RF radioulna & 0.019 & 0.015 & +27\%\\[0pt]
RF metacarpal & 0.008 & 0.006 & +33\%\\[0pt]
RF hoof & 0.005 & 0.005 & +0\%\\[0pt]
RH femur & 0.050 & 0.053 & -6\%\\[0pt]
RH tibiafibula & 0.023 & 0.021 & +10\%\\[0pt]
RH metatarsal & 0.013 & 0.011 & +18\%\\[0pt]
RH hoof & 0.005 & 0.003 & +67\%\\[0pt]
\hline
\textbf{TOTAL} & \textbf{1.226} & \textbf{1.091} & \\[0pt]
\hline
\hline
\end{tabular}
\end{table}


\subsection{Issue 1: Beam Hardening}
\label{sec:orgca94bca}

\begin{figure}[htbp]
\centering
\includegraphics[width=18cm]{./figures/cupping_results.pdf}
\caption{\label{fig:cupping_results}Effect of beam hardening on calculated mass moment of inertia of a PTFE cylinder. (A) Repeat of radial profiles (as in Fig. \ref{fig:cupping_methods}C) for different beam hardening strengths. (B) Calculated mass moments of inertia, unscaled (\(\times\)) and scaled \(\circ\). In this cylinder, \(I_z\) is always larger than \(I_x, I_y\) which are approximately identical. The solid, dark red line indicates the theoretical (true) \(I_z\) of the PTFE cylinder, the dashed red line marks the true value of the other principal inertial axes. The more beam hardening \(k\), the lower the (unscaled) mass moment of inertia. Noteworthy, at \(k=0\) the inertia values match the true values. However, this is only true for the unscaled curves; when scaling to make the calculated mass of the cylinder match the actual mass (i.e. normalizing mass, see text) the correct inertia is reached at an arbitrary, non-zero \(k\), and the calculated inertia increases with beam hardening due to the normalization.}
\end{figure}

To evaluate what effect beam hardening will have on the calculated mass moment of inertia, we simulated beam hardening on a well described PTFE cylinder within the scan volume.

The only reliable recovery of the true mass moment of inertia (Fig. \ref{fig:cupping_results}B, horizontal lines) is found to be if there is exactly zero cupping.
This generally holds for homogeneous objects: moment of inertia is determined by the geometric distribution of mass elements around the COM; iff the relative mass of all mass elements are well approximated by the density regression (which is only and exclusively the case in homogeneous objects), and iff cupping is negligible, then we can calculate appropriate \(I\) values.
However, if the object is non-homogeneous (as a femur, or a cylinder with considerable cupping), then the relative density of voxels are not appropriately covered, and \(I\) will be incorrect.
If there are \textbf{any beam hardening or streak artifacts}, which is non-trivial to remove from FBP reconstructed scans, \textbf{the moment of inertia will be incorrect}.
Mass scaling, which was suggested as a strategy to recover density distribution, will produce erroneous moments of inertia if artifacts are present.
Attention should also be put on the units of the calculated mass moments of inertia: it is in the order of \(10^{-7} kg m^2\) for this cylinder.
This is small, justifying extra attention to numeric accuracy of computational methods as well as sources of physical inaccuracy (e.g. COM miscalculation).

We conclude that the chance of getting an accurate measurement for mass moment of inertia from conventional CT scans is low under the influence of common error sources.



\subsection{Issue 2: Metal Bead Artifacts and Chemical Composition}
\label{sec:orgd9f8ffb}

\begin{figure}[htbp]
\centering
\includegraphics[width=12cm]{./figures/metal_artifacts.png}
\caption{\label{fig:metal_artifacts}Metal Bead Artifacts, marked with orange arrows. Despite the use of an aluminium filter to harden the x-ray beam, metal beads produce conspicuous, star-shaped streaks on the scan reconstructions. The reason that the center of the metal beads appears black is a clipping setting during image preprocessing (intentional, do ensure they do not contribute to the total mass).}
\end{figure}

Another typical kind of artifacts in CT scans appear around metal beads (Fig. \ref{metal_artifacts}).

Metals are chemical elements with overproportionally higher x-ray attenuation than carbohydrate compounds.
They absorb a large fraction of the incoming radiation, demanding the use of hight energy for sufficient penetration to avoid shadows arising from missing information in the projections, which can be detrimental to contrast on softer tissue.
The shadows appear as ``streaks'' or ``starbursts'' on CT scans, they complicate segmentation, and are unwanted in XROMM visualizations \citep{Brainerd2010}.
As with other artifacts, metal bead artifacts have their source in limitations of the tomographic method (limited dynamic range, rotationally symmetric scan geometry), and might be corrected by appropriate counter-measures.
Beam hardening reduction by using metal filter plates can improve dynamic range and contrast (at the cost of exposure).
The use of a helical scan geometry in a cone-beam CT should reduce the length and contrast of streaks, because projections from above or below the bead elevation level will project through the tissue immediately behind the bead.

Though it is pronounced on metal beads due to their spatial confinement, the problem is not restricted to metal.
Streak artifacts also appear between multiple regions of dense bone or strongly absorbant solute (i.e. water), especially if two such regions align along the beam trajectory in the projection geometry.


Bead artifacts are a kind of ``beam hardening'', taken to the extreme: the spectrum of the photon ensemble changes along the projection trajectory, and much more so if x-ray interactions with matter are more numerous.
They are more numerous in some materials than others (irrespective of density, e.g. formol): chemical composition is key.


\subsection{Sensitivity Analysis}
\label{sec:org768a505}

\begin{table}[htbp]
\caption{\label{tab:sensitivity}Sensitivity Analysis of the Mass Moment of Inertia calculations of the femur scan. For reference, the actual femur weighed \(5.271 g\), had a length of \(46 mm\), volume of \(4.637\times 10^{-6} m^{3}\) and an average density of \(\approx 1140 kg m^{-3}\).}
\centering
\begin{tabular}{|l|l|c|c|c|c|c|}
\hline
\textbf{parameter} & \textbf{change} & \textbf{\(\sum_{i}m_{i}\)} & \textbf{\(\Delta COM\)} & \textbf{\(I_{xx}\)} & \textbf{\(I_{yy}\)} & \textbf{\(I_{zz}\)}\\[0pt]
 &  & \([g]\) & \([px]\) & \([\times 10^{-6} kg m^2]\) & \([\times 10^{-6} kg m^2]\) & \([\times 10^{-6} kg m^2]\)\\[0pt]
\hline
\hline
\textbf{(reference)} & none & 6.38 & 0 & 0.52 & 1.36 & 1.67\\[0pt]
\hline
\textbf{regression \(a\)} & \(+5\%\) & 6.12 & <1 & 0.50 & 1.31 & 1.60\\[0pt]
\textbf{regression \(b\)} & \(+5\%\) & 6.07 & 0 & 0.49 & 1.30 & 1.59\\[0pt]
\hline
\textbf{mass \(\sum_{i} m_{i}\)} & \(+0.26 g\) & 6.70 & 0 & 0.55 & 1.43 & 1.75\\[0pt]
\hline
\textbf{COM \(x\)} & \(+2.5 mm\) & 6.38 & 50 & 0.52 & 1.40 & 1.71\\[0pt]
\textbf{COM \(y\)} & \(+2.5 mm\) & 6.38 & 50 & 0.56 & 1.36 & 1.71\\[0pt]
\textbf{COM \(z\)} & \(+2.5 mm\) & 6.38 & 50 & 0.56 & 1.40 & 1.67\\[0pt]
\hline
\textbf{regression type} & expon. & 4.96 & 5 & 0.42 & 1.09 & 1.34\\[0pt]
\hline
\hline
\end{tabular}
\end{table}


To put all the aforementioned issues into perspective, we calculated how a change in one parameter affects the outcome of the calculation of inertial properties (Tab. \ref{tab:sensitivity}).

Moderate changes in the density calibration regression directly influence the mass, and thereby the moment of inertia; even the COM is mildly affected.
The effect on \(I\) is in the order of \(7-8\%\).
Adjusting the mass has a similar effect as a change in regression parameters, and a \(5\%\) mass increase causes a \(\Delta I\) of more than \(8\%\).
Shifting the COM alone does affect inertial parameters depending on the shift direction, which is expected (Steiner theorem).
The effect of the notable shift (\(5\%\) of the femur length) is a change in \(I\) of about \(4\%\).
This is an artificial adjustment, changing the COM without a change in mass, but it does help to estimate errors in an inverse dynamics workflow, where a COM misplacement will also affect a joint wrench according to the Steiner theorem.
Mass changes seem to affect all inertia axes alike, whereas the error due to COM shift affects the axes differently.
Finally, the exponential regression leads to a moderate shift in COM (\(0.25 mm\)), but a considerable underestimation of mass (\(-29 \%\)) and moment of inertia (\(-32 \%\)).

These simulations confirm that moment of inertia crucially depends on the mass that is assigned to each voxel in the CT data set; the relative error in the outcome can even exceed the input inaccuracy.


\begin{figure}[htbp]
\centering
\includegraphics[width=16cm]{./figures/probabilistic_regression.png}
\caption{\label{fig:probabilistic_density}Probabilistic density regression result. Traceplot showing the posterior distributions of model parameters (left) and sampling traces (right). The ``uncertainty'' (\(\varepsilon\)) quantifies the hierarchical model residual, i.e. the total uncertainty remaining after convergence. The colored distributions for ``sigma'' are the material-wise uncertainties in the gray values.}
\end{figure}

To also get an estimate how variable the actual gray value observations were, a probabilistic linear regression was applied (Fig. \ref{fig:probabilistic_density}).

The intercept was sampled at \(0.173\pm 10^{-4}\), and the slope was \(1.6\times 10^{-4} \pm 10^{-7}\), both consistent with the least squares regression.
The result for the model residual \(\varepsilon\) is \(0.024 \pm 0.008\), which is \(14\%\) of the intercept.
Note that reference to the intercept here is in line with using a percent adjustment of the regression parameters above; a more realistic estimate for ``percent mass change'' would be the average gray value of the femur (\(0.39 \longrightarrow 6.3\%\)); that value is also in the range of the mass adjustment simulated above.
Furthermore, the material-wise residuals \(\varepsilon_{j}\) turn out to increase with increasing density, yet that overall trend is not systematic for each material.
The specific uncertainty for air is lowest of all materials with \(0.011\), whereas the specific uncertainty for PTFE is \(0.021\).
The highest uncertainty is measured for HMPE (\(0.23\)) and polycarbonate (\(0.27\)), potentially indicating that these objects were less homogeneous than for example PTFE.
Overall, one can attribute roughly half of the model residual to uncertainty in the slope.

These results indicate that the \(5\%\) error margins used above are rather conservative estimates: the actual uncertainty is higher, and affects both the slope and the intercept.


\section{Discussion}
\label{sec:org3c02141}
\subsection{Density and Attenuation}
\label{sec:org5e34015}
In this chapter, I challenge the common misconception that structural information from CT images can be easily converted to a density distribution.
Humans think of objects in terms of their mass because that is what we are confronted with in our everyday experience of the physical world.
And the structures which we recognize in CT visualizations resemble that experience.
However, the raw images quantify attenuation, which is a fundamentally different phenomenon.


And even the voxel-wise attenuation cannot be accurately extracted, for the following reasons.
X-ray radiation used in commercial CT scanners is often polychromatic.
Absorption is wavelength-dependent in a non-trivial way.
The whole process contains stochastic aspects (e.g. scattering).
Scientists tend to introduce the constraint that chemical composition must be homogeneous within samples, yet that constraint is neither realistic, nor practical.
Finally, the effects some qualify as scan artifacts seem to be an issue, but as argued above they might just be the manifestation of the inappropriate modeling of a normal physical process.


\subsection{Miscalculated Mass}
\label{sec:org737d0a4}
The thorough measurements reported above confirm that the outcome of a CT scan cannot be directly related to physical density.
We followed the strategy of others, using a kind of calibration regression to convert CT values to density \citep{Mull1984,Phillips1997,DuPlessis2013,Durston2022,Fath2023}.
Our measurements show severe differences between the actual density and the calculated values, with errors of more than \(20\%\) of the actual mass.


Neither plastic parts, nor the calibration cylinders we used provided an accurate reference for the density of the test object.
The bone mineral density phantoms arguably serve a different purpose (calibrating \emph{mineral} density), but resemble the compact parts of the bone in their chemical composition.
Yet it is not only the chemical composition of the scanned object, but also the spatial arrangement of its components, because the trajectory of the x-ray beam affects attenuation (``self-filtering'').
It is insufficient to just assume ``comparable chemical composition''.
Calibration object should also have the same shape and spatial arrangement as the object of interest.
The ideal object to calibrate an unknown femur would thus be an identical femur of known mass distribution \citep[which is actually attempted in clinical tomography, cf.][]{Lennie2021}.


The reason, as illustrated in the introduction, is that photons of different wavelengths have specific absorption probabilities.
The spectrum of the beam changes at each instance in the tissue.
The most obvious example of this is beam hardening, and I showed by a simple virtual simulation that beam hardening leads to a systematic under-estimation of mass and moment of inertia (Fig. \ref{fig:cupping_results}).
In a more extreme case of this, radio-opaque metal beads will absorb almost all passing photons (``photon starvation''), leading to problems with conventional reconstruction algorithms which appear as starburst artifacts.


Recent developments attempt to improve the reconstruction algorithm \citep{Six2019,Frenkel2022,Yang2021,Batenburg2011}, but their application requires prior information and extra work.
Others might eagerly await ``spectral CT'' \citep{Liu2023}, yet technical and economic challenges are still limiting the capability and availability of such machines.


\subsection{Non-Intuitive Inertia}
\label{sec:org267fc97}
It is perfectly normal that physical measurements are subject to measurement inaccuracy.
However, the error range must be quantified and reported.

Above, we demonstrated an analytical assessment of which error contributions should be expected, as well as a numerical sensitivity analysis.
We also quantified the actual regression uncertainty with probabilistic statistical modeling.
The results enable an evaluation of the relative importance of different parameters for the accuracy of the outcome.
For example, changes in the mass or in the regression outcome directly influence the moment of inertia, at a rate higher than the input error.
A COM miscalculation might cause errors of less magnitude, but those are not necessarily isotropic.


Yet by far the greatest effect was due to a change in the regression type, switching from linear to exponential.
Neither of these is accurate, yet in the absence of a model which covers the nature of attenuation (which would rather have to adjust the reconstruction algorithm than the regression), there is no other reasonable option.
The exponential fit seemed plausible from the BMD phantoms.
The majority of observed femur gray values were not far off the exponential regression line.
Nevertheless, the calculated moment of inertia was about \(30\%\) off the reference.
Note that this error is the sum of both an over-estimation in the linear reference regression, and an under-estimation of \(I\) in the exponential attempt.
Also, the high percentage difference is explained by the fact that moment of inertia values are relatively small in baseline magnitude.
A shortcoming of the present study is that we are unable to determine a ``ground truth'' inertia for the test object, lacking sufficiently accurate physical measurement apperature and expertise.


Nevertheless, our findings have practical application.
Any regression should be as accurate as possible (optimal exposure and dynamic range, noise reduction e.g. by more averaging, more/diverse calibration objects).
A probabilistic regression is favorable, since it yields error margins for the regression parameters.
With those, a sensitivity analysis can and should be performed.


I also evaluated scaling voxel-wise density so that the mass matches the known total mass is not an option: it can amplify issues from beam hardening (Fig. \ref{fig:cupping_results}) or from marker streaks and thus make the outcome more unpredictable.
The rationale behind scaling was that, for moment of inertia, the distribution of densities is relevant.
Scaling masses is identical to adjusting the intercept parameter in the linear regression (Tabs. \ref{tab:regressions}, \ref{tab:sensitivity}).
An alternative, which we did not attempt, would be to adjust the regression slope so to match the mass checksum.
However, this has conceptual flaws.
Firstly, the resulting modified regression would not correctly fit any of the calibration objects any more, and the question arises which of the measurements is ``more trustworthy'' or ``more true''.
Second, slope adjustment over-values the ``air'' observation: ``air'' has a negligible density, yet might have a non-negligible attenuation (which is visible from its gray value distribution; x-ray flux strongly depends on source-detector-distance in non-vacuum).
This (energy-dependent) air issue might actually explain the aforementioned systematic errors in the supplemented regression plots of \citet{Durston2022}.
Third, again, there is no mechanistic reason to assume a linear relation; the polymer plastics of identical chemical composition are a rare corner case, most other series of arrays could be non-linear.


\subsection{Inertia Components}
\label{sec:orgaad4540}
To my knowledge, this study is the first to perform a detailed sensitivity analysis of the CT density calibration.
Sensitivity is high, so are the measured error margins, which implies that potential inaccuracies are potentially fatal for the derived measurements.

Yet whether or not an error margin is ``fatal'' also depends on the field of application.
The purpose of the inertial estimation is to calculate joint moments in an inverse dynamic workflow.
Joint moments of a limb in ground contact are dominated by the ground reaction force, i.e. by the gravitational load of the animal.
For grounded limbs, the inertial properties of the limb segments themselves are negligible, and studies of high joint load will have little to worry about.

In contrast, when calculating the inertia of limbs in the swing phase, the inertial components dominate the joint moment calculations.
In those cases, measurement inaccuracy should receive a close inspection.


There is another aspect to the miscalculation of inertials: the ``rigid body'' assumption.
Segments \emph{in vivo} are not rigid.
Anisometric muscle contractions will shift the COM and might even rotate the inertial axes.
Changing the configuration of a limb (e.g. flexing a leg during swing phase) is known to have a profound (usually favorable) effect on the compound limb inertia.

Finally, in vivo movements are often restricted to one or few axes.
If a femur swings forward, it might slightly rotate around its long axis, however the most relevant rotational movement is induced around the mediolateral axis.
Thus, that inertial component demands close attention.


\subsection{Lack of Alternatives}
\label{sec:orgcbe870c}
The femur CT scan used in this experiment was recorded with all necessary diligence, under ideal conditions, with good image quality and resolution.
For example, a beam hardening filter was used, sample mounting was flawless, contrast and dynamic range were optimized.
Still, we see a distribution for gray values which hinders determining the femur mass at an accuracy better than \(10\%\) (see probabilistic regression).

The proximal conclusion would be to reject the density estimation from CT data.


However, such an attitude demands the evaluation of alternatives.
The mass can be measured accurately with sufficiently fine scales, yet dissection of segments to determine segment-wise mass literally introduces fuzzy edges.
Center of mass can be estimated by pendulum methods, yet in addition to the dissection issues, the problem of accurately relating the COM to an anatomical reference frame arises.
The conventional instrument to measure moment of inertia is the trifilar pendulum.
As \citet{Durston2022} have demonstrated, this method is far from trivial, requires a well-prepared setup, and still suffers from limitations (e.g. that all axes have to be measured separately).
Hence, it can be doubted that the accuracy of these methods exceeds that of CT-based approximations.

Thus, the alternatives to CT scans for the present purpose might be \emph{better} CT scans.
Projection images are monochrome, not monochromatic, and the step towards ``color CT'' might improve density estimation by incorporating models of chemical composition.
This will doubtlessly require new algorithms, most likely algebraic ones, which iteratively adjust a model volume to match simulated and real CT projections (which would then be polychromatic).
All those are promising developments that would certainly impact the accuracy of XROMM experiments and inverse dynamic models.


\clearpage
\section{Summary}
\label{sec:org65e7851}
At the end of this chapter, it should be clear that the only people using CT images to calculate inertial properties would be Biologists who are agnostic to the basic principles of physics.
This includes the author.
Tomographic images depict attenuation, not physical density, and though these are occasionally correlated, they remain fundamentally different properties of matter.
Above, I discussed potential flaws in the CT-based approximation of mass, center of mass, and mass moment of inertia, as well as corner cases in which the calculation might co-incidentally yield appropriate results.
In all other cases, the calculation will certainly yield wrong results.
The magnitude of the error is non-negligible, since some axes of inertia in balance equations have very small base magnitude and are therefore susceptible to faults.


However, with all that objection put to paper, it must be stated that there is no effective alternative to the use of CT scans.
There is no ``density tomography'', and common alternative methods suffer from equally large error margins for other reasons (e.g. dissection inaccuracy, non-static configuration during movements).

Using attenuation values to calculate densities might be about as accurate as setting \(\pi \equiv 3\) in physics classes (a totally valid thing to do, if at the same time setting \(g \equiv 10\)).
Yet when knowing and avoiding typical pitfalls (beam hardening, metal beads, symmetries), and when keeping track of error magnitudes (by performing a sensitivity analysis, involving e.g. Gaussian error propagation and probabilistic regression), the calculated inertials might be a just sufficiently accurate approximation of the true values.
