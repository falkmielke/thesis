\clearpage
\section{Abstract}
\label{sec:org6a664ae}
An essential part of the inverse dynamic workflow is the measurement of inertial properties, in particular mass, center of mass, and mass moment of inertia.
In the past, these have often been approximated by assigning average density values to limb segments.
Recent studies have explored the potentially more accurate retrieval of inertials from computed tomography (CT) data, which would work by associating voxel gray values of the CT image stack with densities.
However, CT images actually depict attenuation, which is a different attribute of matter and only roughly correlates with density.
Though this was known since the early days of CT, the temptation of easily retrieving inertials has recently led researchers to re-attempt linear approximation methods.

In this study, I document my own attempts in retrieving density values from CT images.
Though a conversion is possible by using a regression, I demonstrate that the resulting mass and inertia are highly erratic.
By performing a thorough sensitivity analysis, and by simulating common CT artifact, I can quantitatively evaluate what effect uncertainties in the regression have for the outcome measures.

The results are alarming: in the test case, mass is overestimated by more than a fifth, and error propagation indicates that this has an even worse effect on moment of inertia, which directly translates to errors in inertial dynamics.
However, there is no reason to expect alternative, conventional methods to be more accurate.
Thus, though CT images are not ideal for the purpose, they might be the best option available in most cases.
In conclusion, I emphasize the value of a sensitivity analysis, and point at promising research progress of others which might lead to improving CT reconstructions for the purpose of estimating material characteristics.


\clearpage
\section{Introduction}
\label{sec:org2c6236f}
\subsection{Computed Tomography and Density}
\label{sec:org802d06c}
Since the advent of x-ray imaging, people are intrigued by the ability to see the inner structure of objects and living creatures (such as the famous hand of Röntgen's wife, ``Über eine neue Art von Strahlen'', 1895).
This desire even increased by the development of computed tomography \citep[CT,][]{Beckmann2006,Hounsfield1973}, a set of techniques which enable the reconstruction and visualization of three-dimensional structural images.
The transmission images obtained via high energy electromagnetic radiation often serve to answer qualitative questions (e.g. whether a bone is fractured).
Quantitative questions are obvious with regard to the shape of a scanned structure (e.g. the length of a fracture, the shape of a bone, trabecular and cortical micro-/architecture).
However, researchers have been struggling with the quantitative extraction of material properties from CT data.
The material property of primary interest is physical density; from a given density distribution, other relevant inertial properties can be calculated.


Researchers suggested early on to relate the gray value of CT images to density \citep{Mull1984,Phelps1975}, yet it was immediately noted that the relation is nothing more than a correlation which only holds under specific circumstances.
Even these pioneer works acknowledge that there must be discrepancies between real and x-ray-derived densities, which are often associated with (i) the polychromatic character (source spectrum) of the used radiation, (ii) chemical composition (absorption spectrum), and (iii) scan artifacts.

These issues demand a detailed explanation, below.
Notwithstanding this list of known problems, people have repeatedly attempted to extract density and inertial properties from CT scans  \citep[][as well as the present study]{Phillips1997,DuPlessis2013,Durston2022}.
The purpose of this chapter is to explore whether or not (or under which circumstances) CT gray values can be used to estimate density distributions, and thereby mass and other derived inertial properties.
I will start by giving a brief intro to crucial aspects of CT scanning technology, before reviewing similar attempts by others, then introducing a simple experimental setup to measure dynamics of an excised piglet femur, and with that re-attempting the extraction of inertial properties.
\subsection{Emission and Absorption}
\label{sec:orga7169e0}
Visible light and x-ray radiation are, to a large degree, analogous.
The reason for this is that both are electromagnetic radiation; \chng{they differ in wavelength, thereby energy, and thereby in the way they interact with matter}.
To illustrate what is happening with x-ray light during a CT measurement, I will use the analogy of visible light.



Most plants have evolved to be green; their cells hold organelles with the green pigment chlorophyll, which absorbs some non-green (e.g. red) wavelengths of light very well (Fig. \ref{fig:spectrum}, green curve).
\begin{change}
Light are photons, ``elementary particles which are the quantum of the electromagnetic field'' (\href{https://en.wikipedia.org/wiki/Photon}{Wikipedia: photon}).
Filters let through some of them, while prohibiting others from passing.
Think of party lights or colored window sheets: a light source is placed behind one or more filter sheets which block most colors and let through certain others.
In case of a green filter sheet, relatively more green photons pass the filter, the filtered spectrum will appear green to our visual perceptive system.
In plants, the ensemble of non-absorbed photons also appears green to us, which is why leaves generally appear green to our view.
\end{change}
But leaves only appear green if there is a green content in the incident light!
Green leaves will look red under red light (and ``darker'', i.e. lower number of reflected photons than for an equal input intensity of green light).
\begin{change}
If you want to reduce the chance being seen while looking for something in a dark forest, better use a red flashlight.
\end{change}


\begin{figure}[p]
\centering
\includegraphics[width=13cm]{./figures/spectrum.pdf}
\caption{\label{fig:spectrum}\textbf{Spectra.} The horizontal axis shows all wavelengths \(\lambda\) (\(nm\)) of relevance (in this case the range of visible light, for illustration). Related to wavelength and therefore viable axis label alternatives are frequency (units: \(Hz\)) and photon energy (units: \(eV\)). The vertical axis shows photon occurrence probability, or normalized intensity \(I\), or the intensity difference \(\Delta I\) for the absorption spectrum. Black curve: emission spectrum of a white LED \citep{Tanabe2005}. Green curve: the approximate absorption spectrum of chlorophyll A \citep{Zscheile1934}. The chlorophyll will hardly get excited by the LED, yet the plant will look bright and green.}
\end{figure}


To characterize light which is emitted, reflected, or absorbed, one can plot a spectrum (Fig. \ref{fig:spectrum}).
Red LEDs are an alternative to red filters: they immediately produce a spectrum which is biased towards low frequencies.
One can assess that red LEDs have a different \textbf{source spectrum or emission spectrum} than white LEDs or ``vintage'' (tungsten) light bulbs.
If the light from a white source is filtered (e.g. by a red sheet of thin, translucent plastic), the spectrum is altered.
A red LED viewed through a green-pass filter (e.g. a leaf) might appear to be ``off'', when really power is ``on'', because the majority of photons are absorbed by the filter.

In fact, light hitting \emph{any} object will change its spectral composition, depending on the material's \textbf{absorption, transmission, and reflection spectrum}.
This is also a filtering process: the outgoing light will depend on the incoming spectrum and the material properties of the object.
Plant leaves will almost entirely absorb a narrow band of incoming red light.
Interactions of photons and matter crucially depend on the energy/frequency/wavelength of the photons, and the energetic/vibrational/resonant molecular properties of the matter.
A (differential) spectrum is a way to depict a wavelength dependence.


\begin{figure}[p]
\centering
\includegraphics[width=10cm]{./figures/Berger2018.png}
\caption{\label{fig:xray_emission}\textbf{X-ray emission spectrum.} An x-ray source will emit photons with a variety of energies (``colors''). Bremsstrahlung will cover a wide range of photon energies, whereas discrete peaks are caused by specific emission processes in the target material. \citep[taken from ][creative commons license]{Berger2018fig8}.}
\end{figure}

\begin{change}
Crucially, the interaction of x-rays (i.e. high-energy, low wavelength photons) with matter differs from that of visible light and matter.
Hence, the analogy between visible light and x-rays only partly holds.
X-rays \citep{Berger2018,Buzug2008} are also photons, just at a different wavelength (range \(10\ pm\) to \(10\ nm\)).
An x-ray detector is a complex sensor device, similar in function to digital camera sensors, which will detect any light which is not absorbed or scattered by the sample or air on its way through the scanner chamber.
\end{change}
X-ray sources are ``targets'', i.e. anodes made of certain metals (Tungsten, Molybdenum, \ldots{}), shot at with an electron beam which is evoked by applying voltage (e.g. \(60\ kV\)) to the warmed-up anode and cathode.
When hit by electrons, the target will emit x-ray photons of a specific composition of energies (spectrum, Fig. \ref{fig:xray_emission}).
The source emission is generally \textbf{polychromatic}, i.e. consisting of multiple colors/energies (just as the spectra in Figs. \ref{fig:spectrum} and \ref{fig:xray_emission}).
Most household CT scanners (in contrast to synchrotrons) have a polychromatic source.
On the other hand, x-ray detectors usually produce monochrome images, but are not monochromatic!
Much like a monochrome digital camera photo cell measures light intensity of any wavelength within the visual range (restricted to \emph{visual} by another filter); in contrast, a truly monochromatic detector would detect only a specific wavelength.
They integrate intensities over a wide range of wavelengths (in a specific way that could be measured as the sensitivity spectrum of the scintillator\footnote{The scintillator is the device which enables detection of x-ray, by converting x-ray light into visible photons which are then detected by a sensor array.}).
There is ongoing development on the frontier of ``spectral CT'' \citep{Liu2023}, yet resolution (spectral and spatial) are currently still below par (whereas price is not).


\begin{change}
Once set on their path from within the x-ray source, x-ray photons interact stochastically with matter they encounter on their trajectory (photoelectric effect, Compton scattering).
\textbf{``Attenuation''} is the term to describe that not all photons reach the detector, i.e. the detected x-ray intensity is lower than the incident intensity.
The probability of either of the possible interactions depends on (i) the wavelength (photon energy), (ii) the elementary composition of the material (absorption spectrum, K-edges), and (iii) the trajectory of the photon (thickness of the material, angle of incidence).
\end{change}

The varying degrees of attenuation, when measured from multiple incident angles for 3D view, is what enables the extraction of structural information (Lambert-Beer's Law).
If children place their hands inside a conical light beam, an animal-shaped shadow will be projected onto the wall.
Their hands attenuate the light.
Photos of the hand's attenuation pattern from all possible directions (i.e. rotating the hand) can in fact be used to reconstruct the 3D shape of the hand to falsify the hypothesis that an actual animal was causing the projection.

And attenuation is precisely the property which is thought to correlate with physical density: the higher the density, the higher the attenuation.
Or so it seems.
Yet think of a case in which the specific emission of the source does not match the absorption peaks of the material - remember the example of a white LED not exciting chlorophyll of a green leaf.
Or the opposite case, a substance with an absorption spectrum which is mostly congruent to the emission spectrum of the source.
Examples of substances problematic for x-ray are water and formol, because they absorb a broad range of photon energies within the x-ray range.


To summarize: both in the visual and x-ray range of electromagnetic radiation, emission and absorption are determined by stochastic interactions of \chng{photons and matter}.
Spectra summarize ensemble properties of a given light source or material; differential spectra measure relative absorption.
The filtering properties of matter can be used to acquire images and reconstruct 3D structure, even in the absence of precise spectral information.
\subsection{Scan Artifacts}
\label{sec:org45716c7}
X-ray images do not always look as one would want them to look.
The unfavorable image features are commonly called ``artifacts'' \citep{Triche2019}.
In the opinion of the provocative author, there is actually no such thing as CT scanning artifacts.
The term ``artifact'' implies that there is an unavoidable ``error'' in the measurement, yet instead it can be ascertained that correctly obtained x-ray images are highly accurate.
Any tomographic reconstruction just shows exactly what is measured, convoluted with ideally negligible reconstruction algorithm characteristics.


Acknowledged, some aspects of the measurement might be unfavorable to the observer, because they deviate from the image which that observer expects, based on their personal experience of the real world.
For example, a ``beam hardening artifact'' occurs if absorption in the superficial layers of an object alter the spectrum of the beam on its trajectory, which will affect the virtual representation of the deeper regions \citep{VanGompel2011}.
Low energy photons have lower penetration depth than high energy photons, because they are more likely to interact with matter.
This causes a gradual change of the spectrum, which will shift towards higher average energy along the ray's trajectory through thick material.
An observer will know that a cylinder is homogeneous, and reject the image which shows a radial density gradient.
However, that gradient is in fact a normal manifestation of the actual physical process (the stochastic interaction of electromagnetic radiation and matter, see above).
Curiously, beam hardening can be minimized by pre-hardening the beam with the use of metal filter plates \citep{Triche2019}.

Similarly, ring artifacts stem from sensitivity variations on the image detector, which are technically inevitable (due to constraints of the physical detection process), but can be rectified reliably \citep{Sijbers2004}.
Partial volume effects are caused by finite scan resolution and voxel volume.
Streak artifacts are caused by limited dynamic range and ``photon starvation'' (i.e. beam hardening, again).
All these could be considered properties of the scan, rather than interpreting them as an annoyance.

Another group of so-called artifacts might stem from the choice and limitations of the reconstruction algorithm.
There are iterative/algebraic and analytical reconstruction methods \citep{Gilbert1972,Andersen1984,Feldkamp1984,Geyer2015,Hansen2021}, all of which have their specific limitations.
These algorithms are constantly refined and improved, and specific algorithm variants can already overcome specific scan limitations \citep[e.g.][]{Six2019,Frenkel2022}.
In the future, the question of reconstruction artifacts will rather be one of algorithm choice.
Today, the most common reconstruction algorithm, used in a vast majority of the CT service facilities with cone-beam setups, is ``filtered backprojection'' (FBP, or Feldkamp/Davis/Kress = FDK algorithm).


To summarize: scan artifacts, if one wants to use that label, are perfectly normal.
Some are to a certain degree avoidable, others show intrinsic features of the technical and computational tomography procedure.
Artifacts are interpreted as ``something is not as it is supposed to be'', despite attenuation-based images being close to technical perfection.
Artifacts are in conflict with our psychological image of the real world: metal beads do not look like ``stars'' (radial light rays) to us in the real world, so the image is rejected.
I suspect that the major reason people still perceive artifacts as problematic is that we actually think of matter and physical objects as a distribution of density, i.e. a mass distribution, whereas x-ray scanning really yields a distribution of x-ray attenuation.

On the other hand, if assuming that density cannot be recovered from attenuation images, the only appropriate way to measure an exact mass distribution of an object would be to physically slice it into millions of little voxel cubes and weigh each of them.
Compared to that option, computational tomography is certainly a time-saving alternative.


Yet it must be kept in mind that x-ray images do not quantify density, they quantify attenuation, most often lumped over a spectrum.
\subsection{Density Approximations: Two Case Studies}
\label{sec:orgbe186e0}
The fact that two physical properties (attenuation and density) are fundamentally different things does not imply that researchers cannot use one to measure the other.
Scientists have repeatedly suggested and attempted to convert CT scan gray values to approximate density, which I will discuss on two examples \citep{DuPlessis2013,Durston2022}.


\citet{DuPlessis2013} acknowledge general difficulties of accuracy and repeatability in extracting density from CT data.

They then average gray values of putatively homogeneous blocks of polymer plastic material, apply linear regression which, as they point out themselves, does not appropriately cover two of the measured data points.
The latter problem is attributed to differences in chemical composition (without discussing the known composition of the objects).
The authors then argue that chemical composition can be assessed by performing a Dual-Energy CT scan (DECT), i.e. scanning at two different tube voltages and taking the ratio of gray values.
Note that this mode of execution of DECT is not really ``dual'' energy: the spectrum of energies elicited in a \(60\ kV\) scan is included in the \(230\ kV\) scan; the photons below \(60\ kV\) might even contribute the majority of light in the high energy scan.
A better differential would have been achieved by incorporating metal filters in the \(230\ kV\) scan (just as in beam hardening filter, see above).
Nevertheless, comparing the calibration line and the DECT ratio results, the putative outliers do not stand out more than other regression elements (especially the PTFE sample puts the original regression in doubt: it is perfectly fit by the calibration regression, is a modest outlier on the DECT ratio, despite a special chemical composition containing fluorine).

For a toy pig of unknown plastic material, \citet{DuPlessis2013} retrieve higher than actual mass estimates; they identify chlorine and calcium content as the cause.
On another unknown sample which is assessed to be ``similar enough'' in chemical composition, they retrieve an accurate prediction.
The authors do not quantify or report measurement uncertainty which manifests in gray value distributions, sub-optimal regression goodness of fit and potentially high regression sensitivity (PTFE), or other sources of variability \citep{Macaulay2017}.
Finally, they destroy a toy pig for the study, which cannot be excused.

In a nutshell, \citet{DuPlessis2013} attempt density prediction in a particular use case (homogeneous, chemically identical objects) and suggest a DECT ratio to assess chemical composition.


\citet{Durston2022} attempt a huge leap from there and measure inertial properties of frozen cadaver parts, both conventionally and computationally.
Emphasis: they use whole birds, and their considerable amount of work must be appreciated!

As the authors above, Durston et al. acknowledge the critical assumption of a linear relation between attenuation and density, and consequentially use a simple linear relation as a conversion from CT scan in Hounsfield Units to actual density.
They supplement the calibration regressions, which show systematic errors at close look (the regression line lies tilted compared to the majority of relevant calibration objects; it looks biased by the ``air'' sample point yet that is a valid and important reference point; the authors do not discuss this).
Still, that the linear fit works at all is surprising, given that this study uses tissue phantoms provided for medical CT, all of which will putatively have a slightly different chemical composition (linearity assumption violated).

To validate their results from CT density estimates, the authors apply two approaches.
The first is a comparison of virtual and physical dissection, with regard to segment mass measurements.
The second is a trifilar pendulum ``ground truth'' for one axis of the mass moment of inertia.
There is also a validation of the pendulum method, by applying it to manufactured nylon blocks, yet quantitative error estimates remain vague.

Overall, results of the \citet{Durston2022} study remain superficial considering the author's valuable efforts on this project.
They juxtapose pie charts of segment masses to verify mass distribution, which is far too inaccurate.
They compare the virtually and physically derived dorso-ventral axis of the moment of inertia, and present what must be a clear mismatch, considering the lack of meaningful error margins.
They discuss the influence of partial volume effects on density (which, agreeably, could be a problem with feathers), however that is irrelevant for mass and moment of inertia because volume and density both change if a voxel contains air and tissue.
They present ``moment of inertia distributions'' in various ways, i.e. the contribution of each voxel to a \(\vec{COM}\)-centered \(I\) for an entire articulated skeletal system, which is of little practical relevance (which they confirm themselves, by comparing extended and retracted wing configuration).
It is a good reproducibility control that their data confirms previous findings of segment masses of some bird species.
The article is published in the ``methods and techniques'' section of the Journal of Experimental Biology, therefore fully focuses on the CT method, and does not discuss any further application of the generated data.



This critique of the studies above is harsh, and I apologize for pointing a finger.
Many other studies have lumped the whole scan volume into a single density material, whereas the presented examples explore potential improvements.
Yet there is a unifying feature and a reason for the highlighted flaws in theses and other studies: they are \emph{output driven}, and fall short of discussing the mechanistics of CT imaging.
As shown in these examples, practitioners often simply assume that structural CT data represents physical density, instead of failing to falsify this claim.
The results are studies which yield approximate density distributions, yet fail to quantify the inaccuracy and uncertainty of their quantitative data.


The author of this thesis is no exception.
As the authors of the studies reviewed here, I lacked insight and was driven by good hope when starting the work on the present project.
In consequence, this study is centered around the hypothesis that CT gray values can not be converted to physical density, hopefully highlighting pitfalls to avoid and limitations to be aware of for future researchers.
\subsection{Reductionist Approach}
\label{sec:org3add2ae}
This study follows up on the idea of using a calibrated CT scan \citep{DuPlessis2013}, and adds a biologically relevant test object as well as an evaluation of the error margins \citep{Hughes2010,Arroyave2022,Myers2015}, which is crucial for practical use.
Calibration is attempted with cheap, leftover plastic pieces of known material and density, as well as commercial bone mineral density calibration phantoms.
I chose a dissected porcine femur as the object of interest of which I seek to estimate the density distribution, and thereby total mass, center of mass, and mass moment of inertia (Fig. \ref{fig:femur_scan}).
I also attempted to validate the method on dissected animal specimens \citep[as][but less sophisticated]{Durston2022}.

The hypothesis that CT images are a valid approximation of physical density is already falsified by the theoretical considerations above.
The following relevant questions remain:
\begin{itemize}
\item How far off the true value is the calculated mass moment of inertia, i.e. what is the measurement error?
\item How (much) exactly do known artifacts contribute to the measurement error?
\item Are the cheap plastic parts appropriate calibration options for organic tissue?
\end{itemize}


It should be pointed out that preliminary (naive) results of this study were first presented at the \href{http://mielke-bio.info/falk/posts/26.seb2021}{conference of the Society of Experimental Biology (SEB) in 2021}, one year prior to the \citet{Durston2022} study.


\begin{figure}[p]
\centering
\includegraphics[width=16cm]{./figures/femur_scan.jpg}
\caption{\label{fig:femur_scan}\textbf{Experimental Setup.} A piglet femur was excised an marked with metal bead markers. Together with various calibration objects, the sample was fixed (``mounted'') in a PET bottle to fit into the cylindrical scan volume of \href{https://www.uantwerpen.be/en/research-groups/vision-lab/x-ray-imaging-resear/flexct}{the FleXCT scanner of the University of Antwerp}. The image on the left shows the sample mount, magnified on the inset. On the right is one CT projection of the sample, with the femur with markers clearly visible on top.}
\end{figure}




\FloatBarrier\clearpage
\section{Materials and Methods}
\label{sec:orgd1763a7}
\subsection{The Flying Femur}
\label{sec:org951da81}

As discussed in the previous chapter, the goal of inverse dynamics is to calculate the segment-wise balance of wrenches, thereby elucidating which forces and moments each joint has to handle in a motor task.
The unit of calculation is therefore a segment.
In a reductionist approach, the purpose of this experiment is to perform all procedures and calculations on one extracted rigid part of a segment.

For this purpose, a femur was excised from a piglet specimen which had been used previously in XROMM\footnote{X-ray Reconstruction Of Moving Morphology, Brainerd et al., 2010} experiments. \nocite{Brainerd2010}
The animal was euthanized after successful completion of the experiment.
Procedures have been approved \emph{a priori} by the Ethical Committee of Animal Experimentation, University of Antwerp, Belgium (approval number 2017–25).
The femur was extracted by carefully dis-articulating the parts of the right hindlimb of the piglet, isolating the femur, and removing surrounding soft tissue.

\begin{figure}[p]
\centering
\includegraphics[width=16cm]{./figures/flying_femur_fotos.png}
\caption{\label{fig:flying_femur_fotos}\textbf{The "flying femur" experiment:} an excised piglet femur used in XROMM experiments as an allegory of reductionism. Left: the femur oscillating on a pendulum. Right: installation with a motor capable of moving the femur in a realistic way.}
\end{figure}


The extracted bone underwent the full XROMM and inverse dynamic modelling procedure (Fig. \ref{fig:flying_femur_fotos}).
Metal bead markers, \(0.5\ mm\) standard soldering balls made out of a lead-tin alloy (\(Sn_{63}Pb_{37}\)), were glued to the extracted bone to simulate the typical XROMM necessity of marked bones and facilitate \chng{landmark tracking on videos}.
Biplanar x-ray video recordings were performed on the \href{https://www.uantwerpen.be/3d2ymox}{University of Antwerp's 3D²YMOX system} \citep{Nguyen2021,Sanctorum2020}, with two experimental settings.
In one setting, the femur hung on a long, thin wire pendulum (appearing on camera to be ``flying'').
In a second setting, the bone was rotated to mimic in vivo motion by a motor to which it was attached with a radio-translucent, custom-made plastic clamp.

After the experiment, the femur was subjected to a calibrated scan at a local micro-CT facility (FleXCT, imec-VisionLab, University of Antwerp, Belgium).
Plastic parts for calibration were donated by the mechanical workshop of the university, covering a range of physical density from polypropylene (PP, \(916\ \frac{kg}{m^3}\)) to Polytetrafluoroethylene (PTFE, \(2210\ \frac{kg}{m^3}\)).

In addition to the plastic debris, two professional calibration phantoms (Bruker, USA) were included in the scan.
Those are hydroxyapathite cylinders, immersed in water over night, with a diameter of about half a centimeter and a length of a centimeter.
They calibrate bone \emph{mineral} density\footnote{Bone Mineral Density (BMD) quantifies the amount of inorganic content within a volume of bone tissue. Though sharing similar units with physical density, BMD measures just a fraction of the physical bone density.} of \(0.25\ \frac{kg}{m^3}\) and \(0.75\ \frac{kg}{m^3}\), and they have a \emph{physical} density of \(1254\ \frac{kg}{m^3}\) and \(1485\ \frac{kg}{m^3}\).
Their physical density was measured by dividing the weight (measured with a fine scale) and the volume (from CT scan), after the author had found out that those are different quantities.
Air and tap water volumes in the scan have known densities and complete the calibration series.


Expected outcomes of the ``flying femur'' experiment are twofold.
Firstly, this provides a test case as a reference for actual XROMM and inverse dynamic calculations, just with simpler calculations, but including an ``order of magnitude'' estimate of the moments and forces required to move the bone (not reported herein).
Secondly, the validity of the ``calibrated CT'' method for extracting inertial properties is to be evaluated.
\subsection{Piglet Data}
\label{sec:org6c4e591}
Originally, the femur was part of an animal, and thus also part of a bigger whole set of experiments.
That project involved newborn piglets reared temporarily at the veterinary facilities of the University of Antwerp, and subjected to XROMM recording sessions.
Experiments were approved by the Ethical Committee of Animal Experimentation, University of Antwerp, Belgium (approval number 2017–25) and performed in June and July 2019.

The part of these experiment relevant for this chapter are the CT scans.
Those were not calibrated with the full array of plastic debris; they contained only air, water, and the bone mineral density phantoms.
One of the piglet scans was chosen for the validation of the density acquisition method by comparing known segment- and total weight to the outcome of the CT method.
For this purpose, the animal was dissected both physically and virtually (segmentation, Fig. \ref{fig:piglet_segmentation}) and the results compared.
Segments were weighted individually on a precision scale.
Virtual segments were processed with the density regression procedure to compute segment-wise mass estimates.

\begin{figure}[p]
\centering
\includegraphics[width=12cm]{./figures/virtual_dissection.png}
\caption{\label{fig:piglet_segmentation}\textbf{Virtual dissection (segmentation) of a piglet CT scan.} Example virtual slice.}
\end{figure}
\subsection{Scan Parameters}
\label{sec:orgeba4090}

\begin{table}[p]
\caption{\label{tab:ct_settings}\textbf{CT scan settings} for the calibrated femur scan.}
\centering
\begin{tabular}{|l|c|}
\hline
voltage & \(150\ kV\)\\[0pt]
power & \(55\ W\)\\[0pt]
current & \(\approx 360\ mA\)\\[0pt]
filter & \(1.0\ mm\ Al\)\\[0pt]
detector field & \(1920\times 1896\ px\)\\[0pt]
pixel size & \((0.15\ mm)^{2}\)\\[0pt]
exposure time & \(50\ ms\)\\[0pt]
averages & \(3\)\\[0pt]
projections & \(2879\)\\[0pt]
source-detector & \(800\ mm\)\\[0pt]
source-object & \(267\ mm\)\\[0pt]
binning & \(none\)\\[0pt]
voxel size & \(50\ \mu m\)\\[0pt]
scan duration & \(10\ min\)\\[0pt]
reco value range & \([-0.2, 1.0]\)\\[0pt]
\hline
\end{tabular}
\end{table}

The femur scan was performed on a customized Tescan Unitom XL (FleXCT, University of Antwerp), with appropriate settings (Tab. \ref{tab:ct_settings}).
The \(1.0\ mm\) aluminum filter plate and the relatively high voltage were chosen to reduce beam hardening and metal bead artifacts.
Three ``averages'' indicate that every projection is the average of three scan images from the same angle, which is a common trick to reduce pixel noise.
Scan geometry was set to give sufficient resolution (small voxel size) at reasonable scan duration.
The reconstruction value range was chosen to cover the entire histogram, excluding the metal beads, which gives best dynamic range on the plastic and organic tissue.


The piglet scans for the second experiment were performed at a different facility (Royal Belgian Institute of Natural Sciences, Brussels, Belgium) and with slightly different settings (\(110\ kV\), \(500\ mA\), \(62\ \mu m\) resolution, \(5\) averages).
Piglets were scanned in frozen state and in bags of two individuals at a time, to save time and cost.
Air, water, and the bone mineral density phantoms were available for calibration in these scans.
Resolution was lower, but exposure settings were comparable to the ones on the femur experiment.
\subsection{Inertial Properties}
\label{sec:orgf007cdf}

All code used on this project, including Python implementations of the mathematical formulas below, can be \href{https://git.sr.ht/\~falk/flying\_femur}{found online} (\nolinkurl{https://git.sr.ht/~falk/flying_femur} and \nolinkurl{http://mielke-bio.info/falk/posts/23.ct_density}).

\begin{figure}[p]
\centering
\includegraphics[width=8cm]{./figures/inertial_properties.pdf}
\caption{\label{fig:inertials}\textbf{Calculation of inertial properties of a limb segment (femur) from CT scans.} CT scans are 3D images, consisting of voxels (cubes) each of which is associated with a gray value. If these could be associated with physical density, it would be possible to calculate the mass, center of mass (\(\vec{COM}\)) and mass moment of inertia. In the calculation procedure, voxels are treated as little mass elements \(m_{i}\), which are positioned at a given vector position \(r_{i}\) from an arbitrary CT origin.}
\end{figure}
\subsubsection{CT Segmentation}
\label{sec:orgebee57d}
Of course, if e.g. the mass of a bone is to be calculated, one intends to sum up the mass of only the bone, excluding the mass of the surrounding air or support material.
A preliminary step to calculate inertial properties from CT scans is the segmentation of the scan.
Segmentation is the separation of the ``relevant'' and ``irrelevant'' sub-volumes within the image stack, in this case the bone and the surrounding air or background.
More generally, limb segments which are treated as a unit have to be labeled in dedicated software (e.g. \href{https://www.slicer.org}{3D Slicer}), typically with a kind of ``color brush'' or ``magic wand'' tool; thresholding of gray values, boolean operations, algebraic operations (``filling'' etc.), and other tricks may simplify the segmentation procedure in certain situations.
The outcome is a 3D boolean mask which can be used to distinguish the voxels in the scan which are associated with the bone of interest.

This step might seem trivial, time consuming, yet necessary, and indeed it is all of that.
\subsubsection{Mass}
\label{sec:org324a94c}
The mass of a volume element \(m_{i}\) of a rigid body is calculated as the density \(\rho_{i}\) of that element, multiplied with its volume \(V_{i}\) (Fig. \ref{fig:inertials}).
Summing up all volume elements will give the total mass of the object:
\begin{equation}\label{eqn:mass}
\sum_{i} m_{i} = \sum\limits_{i \in V} \rho_{i} V_{i}
\end{equation}

The crucial part here is to get \(\rho_{i}\), the density per voxel, which is attempted via a regression.
\subsubsection{Center of Mass}
\label{sec:org9444203}
The center of mass is the mass-weighted average position vector of all volume elements of a rigid body (Fig. \ref{fig:inertials}).
For each volume element \(i\), mass \(m_{i}\) is a scalar, the position vector \(r_{i}\) is a three-by-one vector, and their product is summed up for all voxels of the segmented bone.
The result is normalized by dividing the total mass.
The outcome is the three-by-one position vector of the center of mass (\(\vec{COM}\), shorthand for \(\vec{r}_{\vec{COM}}\)).
\begin{change}
\begin{equation}\label{eqn:com}
 \vec{COM} = \frac{1}{\sum_i m_{i}} \sum\limits_{i \in V} m_{i} \vec{r}_{i}
\end{equation}
\end{change}
\subsubsection{Mass Moment of Inertia}
\label{sec:org4aa19cc}
As stated before, mass moment of inertia is a tensor (speak: \(3\times 3\) matrix) which measures the resistance of an object to angular acceleration.
It can be calculated for any rigid body (see Fig. \ref{fig:inertials}) via an integral formula over all volume elements \citep{WikipediaMOI}:
\begin{equation}\label{eqn:mmoi}
 I = \sum\limits_{i \in V} m_{i} \left( \lVert \vec{r}_{i} \rVert^{2}E_3 - \vec{r}_{i} \otimes \vec{r}_{i} \right)
\end{equation}

\ldots{} with the rigid body's volume \(V\) split up into voxels \(i\) which have mass \(m_i\) and position vector \(\vec{r}_{i}\); \(E_{3}\) is the \(3\times 3\) identity matrix and \(\otimes\) the outer product.

Initially, this is always calculated with respect to the \(\vec{COM}\) as reference point, the algorithm usually includes subtracting the \(\vec{COM}\) from the position vectors.
The inertia tensor is a symmetric matrix, and can be brought to a principal form by an Eigenvalue procedure, in which the off-diagonal (``products of inertia'') become zero.
The reference point of \(I\) can be changed by the generalized parallel axis / Steiner's theorem \citep[][p. 245]{Lynch2017}.
\begin{equation}\label{eqn:steiner}
 I_{p} = I_{0} + m \cdot \left( \vec{s}^{T} \vec{s} E_{3} - \vec{s} \vec{s}^{T} \right)
\end{equation}

\ldots{} with \(\vec{s}\) being the shift vector connecting the original and target points.


It is difficult to get an intuition about Mass Moment of Inertia, but classroom demos can illustrate \chng{how} this measure \chng{depends on the geometric distribution of mass of an object} \citep{Lewin801L19,LewinMOI,Lewin801L24}.
\subsection{Density Regression}
\label{sec:org6c4eb7f}
The critical step in the procedures above is the relation of gray values \(\gamma\) and physical densities \(\rho\).
This is the search for an (idealized) function \(\gamma = f(\rho )\), which can predict the CT gray value for any given density.
The inverse, \(\rho = f^{-1}(\gamma )\), can then be used to assign densities to gray values from the scan.
Several regression functions were attempted, based on a guessed relation of the observed gray values of the calibration objects.

The regression was performed in Python, namely \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html}{using the \texttt{scipy.optimize.minimize} function} to minimize the Euclidean difference between observed and fit values with the Nelder-Mead algorithm, tolerance set to \(10^{-16}\) \citep{Gao2012}.
Convergence was supported by setting meaningful parameter start values close to the expected outcome (Tab. \ref{tab:regressions}).

\begin{table}[p]
\caption{\label{tab:regressions}\textbf{Density Regression Functions} and regression start values.}
\centering
\begin{tabular}{|l|l|l|}
\hline
 & function & start values\\[0pt]
\hline
\hline
linear & \(a + b \cdot x\) & \(a=0.175, b=10^{-4}\)\\[0pt]
exponential & \(a+b\cdot e^{c\cdot x}\) & \(a=0.12, b=0.05, c=0.0015\)\\[0pt]
\hline
\hline
\end{tabular}
\end{table}

Ideally, all measured points would lie on either of these functions.
Yet that turns out not to be the case in the actual data.
Subsets of the calibration objects were selected for the regression to be plausible: one group were air and the plastic parts, which approximate a linear relation; the other group were air, water, and the two bone mineral density phantoms, which seem to follow an exponential relation; the sample labeled ``PVC'' was plausible for neither group and excluded.
\subsection{Scan Artifacts}
\label{sec:org025a437}
To estimate the effect of beam hardening artifacts, one of the scanned calibration objects was selected by segmentation and virtually modified.
I selected PTFE, because it has the highest attenuation in the data set and is therefore most prone to suffering from beam hardening.
The PTFE cylinder has almost ideal cylindrical shape, and was well aligned with the scan rotation axis.
The object was segmented and cropped out of the whole scan so that the long axis of the cylinder aligns with the center of the cropped volume.


Beam hardening manifests in a radial attenuation gradient in objects: superficial layers which are reached first by x-ray beams seem to attenuate more (in absolute terms) than internal volume elements, because they receive a higher input radiation; the inner elements are partially shielded by the outer elements.
The attenuation gradient produces a cup-like profile in the gray value images, which is why beam hardening of cylinders is called ``cupping''.
To quantify the amount of cupping, the cylinder was transformed to cylinder coordinates (scikit image/transform/warp polar, \nolinkurl{https://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.warp_polar}), and then flattened by averaging along its vertical (long) axis.

\begin{equation}\label{eqn:expfit}
 \mathcal{I} = a+b\cdot e^{k\cdot r}
\end{equation}

By using an exponential regression \eqref{eqn:expfit} to the exponential part of that cylinder's \chng{intensity profile} \(\mathcal{I}\), one can extract a parameter \(k\) which quantifies the amount of cupping (\(k\) as in the Dutch word ``kopje'').
With that \(k\) known, one can rectify the gray values of the scan by applying a correction factor which would push up the exponential line to a flat constant (obviously not applicable for irregularly shaped objects).

Similarly, one can multiply the intensity values of each voxel in the original volume with an exponential of its distance to the center point.
Depending on the chosen value for \(k\) in that exponential, one can virtually manipulate the amount of beam hardening (Fig. \ref{fig:cupping_methods}).


\begin{figure}[p]
\centering
\includegraphics[width=16cm]{./figures/cupping_methods.pdf}
\caption{\label{fig:cupping_methods}\textbf{Beam hardening artifact simulation.} The top view (A) and side view (B) of a PTFE cylinder with beam hardening strength \(k\) virtually set to \(k=0.5\). (C) Line profile of scan gray values along the blue line indicated in panel A shows the typical ``cupping''. (D) cylinder coordinate transformation of the slice marked by a blue line in panel B gives the gray values along the radial lines \(r\) at angles \(\phi\) indicated in panel A. (E) Top and side view for different simulated beam hardening strengths \(k\).}
\end{figure}


The focus of this virtual experiment is Mass Moment of Inertia.
One might argue that, though the value of the tensor \(I\) depends on the mass, its principal axis orientation only depends on the mass distribution.
Given an expected mismatch of the calculated total mass and the actual measured total mass of the test object, an appealing strategy might be to scale the voxel-wise masses so that total masses match.
I call this strategy ``scaling'', and it is optionally compared to the ``unscaled'' gray values of the simulated PTFE cylinders.
\subsection{Error Propagation}
\label{sec:org86c1d0e}
What magnitude do miscalculations of the moment of inertia have, and how much of a problem is that for calculations of joint moment?
To evaluate this, one has to consider error propagation \citep{Hughes2010,Arroyave2022,Myers2015} with the balance equations \eqref{eqn:moment_error}.
The variable \(I\) only enters the balance equations through the dynamic wrench, \(\vec{M} = I \ddot\theta\).
This function is linear in \(I\), so error propagation is quite simple\footnote{It should not be assumed that the error in measuring \(\ddot \theta\) is zero, but I will here focus only on the \(I\) component.} \citep{Normann2016}:
@@latex:\begin{change}@@
\begin{equation}\label{eqn:moment_error}
\Delta \vec{M} = \sqrt{\left(\frac{d \vec{M}}{d I}\Delta I\right)^2 + \underbrace{\left(\frac{d \vec{M}}{d \ddot \theta}\Delta \ddot \theta\right)^2}_{=0} } = \Delta I \ddot\theta
\end{equation}
@@latex:\end{change}@@


Sensitivity is linear at this level: any percentage of measurement inaccuracy in \(I\) will directly propagate to the joint moment calculation.


Yet the situation is more involved: calculating \(I\) via equation \eqref{eqn:mmoi} is itself subject to errors, by inaccuracy in the mass itself (regression), and by uncertainty in the \(\vec{COM}\) position and therefore the position vectors \(\vec{r}_{i}\) of each mass element.
\begin{change}
\[ \Delta I = \left. \Delta I \right|_{m_i} + \left. \Delta I \right|_{\vec{r}_i} = \sqrt{ \sum_{i} \left[ \left(\frac{d I}{d m_{i}} \Delta m_{i} \right)^2 + \left(\frac{d I}{d \vec{r}_{i}} \Delta \vec{r}_{i}\right)^2 \right] }\]
\end{change}


The direct dependence on mass is visible in equation \eqref{eqn:mmoi}, uncertainty of the mass elements contribute linearly but are added up; errors in the mass of further away mass elements contribute more to the total error of the moment of inertia.
\begin{change}
\[ \Delta I \mid_{m_i} = \sqrt{\sum\limits_{i}\left(\left( \lVert \vec{r}_{i} \rVert ^2 E_3 - \vec{r}_{i} \otimes \vec{r}_{i} \right)\right)^2 \cdot \Delta m_{i} }\]
\end{change}


The partial derivative with respect to \(\vec{r}_{i}\) must be added, because \(I\) is taken relative to the \(\vec{COM}\) (hence, \(\Delta \vec{r}_i = \Delta \vec{COM}\)).
Given that the choice of reference point for moment of inertia can be adjusted by using generalized Steiner, the error in the \(\vec{COM}\) propagates as a Steiner component, equation \eqref{eqn:steiner}, with \(s=\Delta \vec{COM}\).
Yet \(\Delta \vec{COM}\) also depends non-trivially on the relative mass of volume elements, equation \eqref{eqn:com}.
\[ \Delta \vec{COM} = \sqrt{ \frac{1}{\sum_i m_{i}} \sum\limits_{i} \left( \vec{r}_{i} \cdot \Delta m_{i} \right)^2  + \left( \frac{ \sum\limits_{i} \vec{r}_{i} \cdot m_{i} }{\Delta \sum_i m_{i} }\right)^{2}}\]


With this last equation I have outlined how the target parameter, \(I\), ultimately depends logically on mass elements \(m_{i}\) and their position in different ways.


In practical terms, the expected error can be conveniently computed by iteratively changing the variables of interest by a percent, to infer how ``a percent change'' in a parameter effects the result of the calculation.
This is a numerical \textbf{``sensitivity analysis''}.
From the analytical considerations above, it can be seen that the variables to change in the numeric computation are the regression parameters (\(\pm 5 \%\) of their value), the \(\vec{COM}\) position (\(\pm 5 \%\) of femur length, \(0.25\ mm\) in each direction), and the total mass (\(\pm 5 \%\), which are \(0.26\ g\)).
\subsection{Error Estimation}
\label{sec:org3a0fad2}
Calculating the effect of an error is only one part of sensitivity analysis.
The second, equally relevant part is an estimate of how uncertain parameter estimates actually are.
A great way to measure this are probabilistic statistics (Ch. \ref{cpt:statistics}).
Besides yielding outcomes for the intercept and slope, they also allow to estimate the uncertainty of these parameters by sampling a model residual.


To estimate the uncertainty in the density regression, I repeated the procedure in a probabilistic modeling framework \citep[PyMC, Version 5.6,][]{Salvatier2016}.
The linear regression function was almost unaltered:
\[\gamma = a+b\cdot\rho + \varepsilon_{j}\]


In this framework, the residual \(\varepsilon\) is calculated, measuring how much residual difference remains after optimization of the regression parameters.
In a linear regression, \(\varepsilon\) corresponds to an uncertainty in intercept (it affects all calibration objects alike).
Beyond the standard model residual, I introduced \(\varepsilon_{j}\), with index \(j\) referring to the materials used for calibration.
Each material thus receives its own residual, all of which are drawn from a higher level normal \(\varepsilon\) (hierarchical model design).
This helps to account for systematic increase of the gray value uncertainty with increasing density (``widening'' of the distribution), as it was also observed in \citet{DuPlessis2013}.

Hence, this regression serves three purposes:
\begin{itemize}
\item confirm results of the conventional least squares regression
\item provide a measure for the intercept uncertainty (\(\varepsilon\))
\item identify potential slope uncertainties (\(\varepsilon_{j}\))
\end{itemize}


All parameters were approximated by Normal distributions, except for the \(\varepsilon_{j}\) (\texttt{HalfNormal}, i.e. bound to be \(>0\)).
Results from the conventional regression and wide enough standard deviations were used as priors to initialize the sampling.
The posterior distributions were calculated from the last \(4\cdot 2^{12}\) samples (after sufficient tuning of the NUTS sampler, see Ch. \ref{cpt:statistics}).
Gelman-Rubin statistics and trace plots indicated convergence and well-behaved sampling.


One important distinction from least squares regression is that probabilistic regression does not require averaging of the observed gray values.
Hence, the regression took a large number of voxels into account, capturing the variability in gray values of the homogeneous polymer materials.
A subset of \(10000\) randomly selected voxels per material were added to the data array.
Subsetting is necessary because the number of observations adds computational load for the MCMC sampler.


\clearpage
\section{Results}
\label{sec:org0ee8643}
\subsection{Density Calibration Regression}
\label{sec:orgbb3192e}
\begin{figure}[p]
\centering
\includegraphics[width=16cm]{./figures/regression.pdf}
\caption{\label{fig:density_calibration}\textbf{Density calibration of a CT scan.} The polymer materials show a linear relation between density \(\rho\) and CT gray value \(\gamma\) (exception: PVC, excluded from regression), whereas the positions of BMD phantoms (ld: low density; hd: high density) implies an exponential relation. Violins illustrate the distribution of gray values, which appears due to stochastic attenuation, yet each object in the scan is associated with exactly one true (average) density.}
\end{figure}

To establish a transform function which can convert CT gray values \(\gamma\) to physical density \(\rho\) and back, I compared different regression functions which fit the observed gray values of different polymer- and other materials of known densities (Fig. \ref{fig:density_calibration}).

The random plastic parts obtained from the workshop have known densities, and most of them show a linear relation when plotting \(\gamma\) against \(\rho\).
The exception is PVC, which was therefore excluded from the regression (whether it was truly PVC, or a mislabeled different polymer, cannot be assessed retrospectively).
Bone mineral density (BMD) phantoms and water do not fall on that line.
Their gray values imply an exponential relation of \(\gamma\) and \(\rho\).
Both regressions were performed by minimizing least square difference of observations and the target function.
Both the linear and exponential function can be inverted.
The femur of unknown density distribution lies right between the two regression lines, but spans a large range of gray values and densities because of its heterogeneous composition (marrow, cavities, muscle tissue, calcified bone, metal markers and glue).

With the regression outcome, it is technically possible to convert any observed or simulated voxel gray value to putative density values, which applies for example to the femur.
\subsection{Issue 0: Mass Mismatch}
\label{sec:orgad7923c}

To confirm the outcome of the regressions above, I compared the actual mass of the femur (which is \(5.271\ g\)) to the outcome of the digital approximation.

As seen in the gray value distributions (Fig. \ref{fig:density_calibration}), the majority of femur voxels have a gray value which falls right in between the two regression lines.
This propagates to the calculated outcomes.
With the linear regression, the femur mass is calculated to be \(6.38\ g\) (\(+21 \%\)).
In contrast, the exponential regression yields a lower than actual mass (\(4.96\ g\), \(-6 \%\)).
Though it appears that the correct regression would lie somewhere between the exponential and linear attempt, there is no data, nor theoretical justification supporting an intermediate hypothesis.
Neither is there any reason to think that all biological tissues generally fall in between the presented cases: for other samples, composition might be different.

I conclude that, at least for a femur, the regression method is not reliable.

\bigskip
To exclude that problems arose from the special composition of this isolated bone, I applied a similar regression method to more complex data.

In a second experiment, a piglet body was CT scanned, virtually segmented, and analogously physically dissected to see whether the total calculated masses of the virtual segments match their actual counterparts (Tab. \ref{tab:pigletdissection}).
The corresponding scan was performed at a different facility and long before the density calibration experiments, and at the time the only available calibration points were air, water, and two density calibration phantoms.

Although an exponential regression was used (under-estimation in case of the femur), this second density regression leads to a over-estimation of the mass of most of the segments, and the specimen total (average \(+12 \%\)).

The contradicting results highlights how erratic the digital density approximation is.



\begin{table}[p]
\caption{\label{tab:pigletdissection}\textbf{Segment mass verification on an actual piglet specimen.} On the lighter segments, measurement accuracy and rounding are limiting. Note that the total mass at euthanasia was measured \(1.125\ kg\), a slight loss is expected due to condensation and preservation.}
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{segment} & \textbf{CT segmentation} & \textbf{dissection} & \textbf{difference}\\[0pt]
\textbf{masses} & \([kg]\) & \([kg]\) & \\[0pt]
\hline
\hline
head & 0.247 & 0.230 & +7\%\\[0pt]
torso+trunk & 0.666 & 0.580 & +15\%\\[0pt]
LF humerus & 0.033 & 0.027 & +22\%\\[0pt]
LF radio-ulna & 0.016 & 0.014 & +14\%\\[0pt]
LF metacarpal & 0.009 & 0.006 & +50\%\\[0pt]
LF hoof & 0.006 & 0.005 & +20\%\\[0pt]
LH femur & 0.052 & 0.050 & +4\%\\[0pt]
LH tibia-fibula & 0.027 & 0.020 & +35\%\\[0pt]
LH metatarsal & 0.012 & 0.010 & +20\%\\[0pt]
LH hoof & 0.004 & 0.004 & +0\%\\[0pt]
RF humerus & 0.031 & 0.031 & +0\%\\[0pt]
RF radio-ulna & 0.019 & 0.015 & +27\%\\[0pt]
RF metacarpal & 0.008 & 0.006 & +33\%\\[0pt]
RF hoof & 0.005 & 0.005 & +0\%\\[0pt]
RH femur & 0.050 & 0.053 & -6\%\\[0pt]
RH tibia-fibula & 0.023 & 0.021 & +10\%\\[0pt]
RH metatarsal & 0.013 & 0.011 & +18\%\\[0pt]
RH hoof & 0.005 & 0.003 & +67\%\\[0pt]
\hline
\textbf{TOTAL} & \textbf{1.226} & \textbf{1.091} & \textbf{+12\%}\\[0pt]
\hline
\hline
\end{tabular}
\end{table}
\subsection{Issue 1: Beam Hardening}
\label{sec:org23f1d8d}

\begin{figure}[p]
\centering
\includegraphics[width=16cm]{./figures/cupping_results.pdf}
\caption{\label{fig:cupping_results} \chng{\textbf{Effect of beam hardening on calculated mass moment of inertia of a PTFE cylinder.}} (A) Repeat of radial profiles (as in Fig. \ref{fig:cupping_methods}C) for different beam hardening strengths. (B) Calculated mass moments of inertia, unscaled (blue) and scaled (gray); scaling was optional to make the calculated mass of the cylinder match the actual mass (i.e. normalizing mass, see text). In this cylinder, \(I_z\) is always larger than \(I_x = I_y\). Calculated values were normalized by the theoretical inertia of the PTFE cylinder (\(I/I_{true}\); with \(I_z = 4.6\times10^{-7}\) and \(I_x = 3.3\times10^{-7}\)), hence a correct calculation would yield values of \(1.0\). The more beam hardening \(k\), the lower the (unscaled) mass moment of inertia. Noteworthy, at \(k=0\) the inertia values match the expected values. However, this is only true for the unscaled curves; when scaling, the correct inertia is reached at an arbitrary, non-zero \(k\), and the calculated inertia increases with beam hardening.}
\end{figure}



To evaluate what effect beam hardening has on the calculated mass moment of inertia, I simulated beam hardening on a well described PTFE cylinder within the scan volume.

The only reliable recovery of the true mass moment of inertia (Fig. \ref{fig:cupping_results}B, horizontal lines) is found to be if there is exactly zero cupping.
This generally holds for homogeneous objects: moment of inertia is determined by the geometric distribution of mass elements around the \(\vec{COM}\); if (and only if) the relative mass of all mass elements is well approximated by the density regression (which is only and exclusively the case in homogeneous objects), and only if cupping is negligible, then one can calculate appropriate \(I\) values.
However, if the object is non-homogeneous (as a femur, or a cylinder with considerable cupping), then the relative density of voxels are not appropriately covered, and \(I\) will be incorrect.
If there are \textbf{any beam hardening or streak artifacts}, which is non-trivial to remove from FBP reconstructed scans, \textbf{the moment of inertia will be incorrect}.
Mass scaling, which was suggested as a strategy to recover total mass from density distribution, will produce erroneous moments of inertia if artifacts are present.
Attention should also be put on the units of the calculated mass moments of inertia: it is in the order of \(10^{-7}\ kg m^2\) for this cylinder.
This is small, justifying extra attention to numeric accuracy of computational methods as well as sources of physical inaccuracy (e.g. \(\vec{COM}\) miscalculation).

I conclude that the chance of getting an accurate measurement for mass moment of inertia from conventional CT scans is low under the influence of common error sources.
\subsection{Issue 2: Metal Bead Artifacts and Chemical Composition}
\label{sec:org347d0ce}

\begin{figure}[p]
\centering
\includegraphics[width=12cm]{./figures/metal_artifacts.png}
\caption{\label{fig:metal_artifacts}\textbf{Metal bead artifacts,} marked with orange arrows. Despite the use of an aluminium filter to harden the x-ray beam, metal beads produce conspicuous, star-shaped streaks on the scan reconstructions. The reason that the center of the metal beads appears black is a clipping setting during image preprocessing (intentional, do ensure they do not contribute to the total mass).}
\end{figure}

Another typical kind of artifacts in CT scans appear around metal beads (Fig. \ref{fig:metal_artifacts}).

Metals are chemical elements with over-proportionally higher x-ray attenuation than carbohydrate compounds.
They absorb a large fraction of the incoming radiation, demanding the use of higher energy for sufficient penetration to avoid shadows arising from missing information in the projections; yet higher energy can be detrimental to contrast on softer tissue.
The shadows appear as ``streaks'' or ``starbursts'' on CT scans, they complicate segmentation, and are unwanted in XROMM visualizations \citep{Brainerd2010}.
As with other artifacts, metal bead artifacts have their source in limitations of the tomographic method (limited dynamic range, rotationally symmetric scan geometry), and might be corrected by appropriate counter-measures.
Beam hardening reduction by using metal filter plates can improve dynamic range and contrast (at the cost of exposure).
The use of a helical scan geometry in a cone-beam CT should reduce the length and contrast of streaks, because projections from above or below the bead elevation level will project through the tissue immediately behind the bead.

Though it is pronounced on metal beads due to their spatial confinement, the problem is not restricted to metal.
Streak artifacts also appear between multiple regions of dense bone or strongly absorbent solute (i.e. water), especially if two such regions align along the beam trajectory in the projection geometry.


Bead artifacts are a kind of ``beam hardening'', taken to the extreme: the spectrum of the photon ensemble changes along the projection trajectory, and much more so if x-ray interactions with matter are more numerous.
They are more numerous in some materials than others (irrespective of density, e.g. formol): chemical composition is key.
\subsection{Sensitivity Analysis}
\label{sec:org260ea74}

\begin{table}[p]
\caption{\label{tab:sensitivity}\textbf{Sensitivity analysis of the mass moment of inertia calculations of the femur scan.} For reference, the actual femur weighed \(5.271\ g\), had a length of \(46\ mm\), volume of \(4.637\times 10^{-6}\ m^{3}\) and an average density of \(\approx 1140\ kg m^{-3}\). Regression \(a\) and \(b\) are fit parameters (see text).}
\centering
\begin{tabular}{|l|l|c|c|c|c|c|}
\hline
\textbf{parameter} & \textbf{input} & \textbf{\(\sum_{i}m_{i}\)} & \textbf{\(\Delta \vec{COM}\)} & \textbf{\(I_{xx}\)} & \textbf{\(I_{yy}\)} & \textbf{\(I_{zz}\)}\\[0pt]
 & \textbf{change} & \([g]\) & \([px]\) & \([\times 10^{-6} kg m^2]\) & \([\times 10^{-6} kg m^2]\) & \([\times 10^{-6} kg m^2]\)\\[0pt]
\hline
\hline
\textbf{(reference)} & none & 6.38 & 0 & 0.52 & 1.36 & 1.67\\[0pt]
\hline
\textbf{regression \(a\)} & \(+5 \%\) & 6.12 & <1 & 0.50 & 1.31 & 1.60\\[0pt]
\textbf{regression \(b\)} & \(+5 \%\) & 6.07 & 0 & 0.49 & 1.30 & 1.59\\[0pt]
\hline
\textbf{mass \(\sum_{i} m_{i}\)} & \(+0.26\ g\) & 6.70 & 0 & 0.55 & 1.43 & 1.75\\[0pt]
\hline
\textbf{\(\vec{COM}\) \(x\)} & \(+2.5\ mm\) & 6.38 & 50 & 0.52 & 1.40 & 1.71\\[0pt]
\textbf{\(\vec{COM}\) \(y\)} & \(+2.5\ mm\) & 6.38 & 50 & 0.56 & 1.36 & 1.71\\[0pt]
\textbf{\(\vec{COM}\) \(z\)} & \(+2.5\ mm\) & 6.38 & 50 & 0.56 & 1.40 & 1.67\\[0pt]
\hline
\textbf{regression type} & expon. & 4.96 & 5 & 0.42 & 1.09 & 1.34\\[0pt]
\hline
\hline
\end{tabular}
\end{table}


To put all the aforementioned issues into perspective, I calculated how a change in one parameter affects the outcome of the calculation of inertial properties (Tab. \ref{tab:sensitivity}).

Moderate changes in the density calibration regression directly influence the mass, and thereby the moment of inertia; even the \(\vec{COM}\) is mildly affected.
The effect on \(I\) is in the order of \(7-8 \%\) (determined as Euclidean difference in the principal inertia values).
Adjusting the mass has a similar effect as a change in regression parameters, and a \(5 \%\) mass increase causes a \(\Delta I\) of more than \(8 \%\).
Shifting the \(\vec{COM}\) alone does affect inertial parameters depending on the shift direction, which is expected (Steiner theorem).
The effect of the notable shift (\(5 \%\) of the femur length) is a change in \(I\) of about \(4 \%\).
This is an artificial adjustment, changing the \(\vec{COM}\) without a change in mass or mass distribution, but it does help to estimate errors in an inverse dynamics workflow, where a \(\vec{COM}\) misplacement will also affect a joint wrench according to the Steiner theorem.
Mass changes seem to affect all inertia axes alike, whereas the error due to \(\vec{COM}\) shift affects the axes differently.
Finally, the exponential regression leads to a moderate shift in \(\vec{COM}\) (\(0.25\ mm\)), but an underestimation of mass (\(-29 \%\) compared to linear regression; \(-6 \%\) compared to actual mass) and moment of inertia (\(-32 \%\) compared to linear regression).

These simulations confirm that moment of inertia crucially depends on the mass and position assigned to each voxel in the CT data set; the relative error in the outcome will exceed the input inaccuracy.


\begin{figure}[p]
\centering
\includegraphics[width=16cm]{./figures/probabilistic_regression.png}
\caption{\label{fig:probabilistic_density}\textbf{Probabilistic density regression result.} ``Traceplot'' showing the posterior distributions of model parameters (left) and sampling traces (right). The ``uncertainty'' (\(\varepsilon\)) quantifies the hierarchical model residual, i.e. the total uncertainty remaining after convergence. The colored distributions for ``sigma'' are the material-wise uncertainties in the gray values (\(\varepsilon_{i}\)).}
\end{figure}

\bigskip
To also get an estimate how variable the actual gray value observations were, a probabilistic linear regression was applied (Fig. \ref{fig:probabilistic_density}).

The intercept was sampled at \(0.1734\pm 10^{-4}\), and the slope was \(1.6052\times 10^{-4} \pm 10^{-7}\), both consistent with the least squares regression.
The result for the model residual \(\varepsilon\) is \(0.024 \pm 0.008\), which is \(14 \%\) of the intercept.
Note that reference to the intercept here is in line with using a percent adjustment of the regression parameters above; a more realistic estimate for ``percent mass change'' would be the average gray value of the femur (\(0.39 \longrightarrow 6.3\%\)); that value is also in the range of the mass adjustment simulated above.
Furthermore, the material-wise residuals \(\varepsilon_{j}\) turn out to increase with increasing density, yet that overall trend is not systematic for each material.
The specific uncertainty for air is lowest of all materials with \(0.011\), whereas the specific uncertainty for PTFE is \(0.021\).
The highest uncertainty is measured for HMPE (\(0.023\)) and polycarbonate (\(0.027\)), potentially indicating that these objects were less homogeneous than for example PTFE.
Overall, one can attribute roughly half of the model residual to uncertainty in the slope.

These results indicate that the \(5 \%\) error margins used above are rather conservative estimates: the actual uncertainty is higher, and affects both the slope and the intercept.


\clearpage
\section{Discussion}
\label{sec:org675e13b}
\subsection{Density and Attenuation}
\label{sec:orgc903990}
In this chapter, I challenge the common misconception that structural information from CT images can be easily converted to a density distribution.
Humans think of objects in terms of their mass because that is what we are confronted with in our everyday experience of the physical world.
And the structures which we recognize in CT visualizations resemble that experience.
However, the raw images quantify attenuation, which is a fundamentally different phenomenon.


And even the voxel-wise attenuation cannot be accurately extracted, for the following reasons.
X-ray radiation used in commercial CT scanners is often polychromatic.
Absorption is wavelength-dependent in a non-trivial way.
The whole process contains stochastic aspects (e.g. scattering).
Scientists tend to introduce the constraint that chemical composition must be homogeneous within samples, yet that constraint is neither realistic, nor practical.
Finally, scan ``artifacts'' are often described as problematic, but as argued above they might just be the manifestation of the inappropriate modeling of a normal physical process by our limited arsenal of reconstruction methods.
\subsection{Miscalculated Mass}
\label{sec:orgcabf680}
The thorough measurements reported above confirm that the outcome of a CT scan cannot be directly related to physical density.
I followed the strategy of others, using a kind of calibration regression to convert CT values to density \citep{Mull1984,Phillips1997,DuPlessis2013,Durston2022,Fath2023}.
The presented measurements show severe differences between the actual density and the calculated values, with errors of more than \(20 \%\) of the actual mass.


Neither plastic parts, nor the calibration cylinders I used provide an accurate reference for the density of the test object.
The bone mineral density phantoms arguably serve a different purpose (calibrating \emph{mineral} density), but resemble the compact parts of the bone in their chemical composition.
Yet it is not only the chemical composition of the scanned object, but also the spatial arrangement of its components, because the trajectory of the x-ray beam affects attenuation (``self-filtering'').
It is insufficient to just assume ``comparable chemical composition''.
A calibration object should also have the same shape and spatial arrangement as the object of interest.
The ideal object to calibrate an unknown femur would thus be an identical femur of known mass distribution \citep[which is actually attempted in clinical tomography, cf.][]{Lennie2021}.


The reason, as illustrated in the introduction, is that photons of different wavelengths have specific absorption probabilities.
The spectrum of the beam along its trajectory changes at each instance in the tissue.
The most obvious example of this is beam hardening, and I showed by a simple virtual simulation that beam hardening leads to a systematic under-estimation of mass and moment of inertia (Fig. \ref{fig:cupping_results}).
In a more extreme case of this, radio-opaque metal beads will absorb almost all passing photons (``photon starvation''), leading to problems with conventional reconstruction algorithms which appear as starburst artifacts.


Recent developments attempt to improve the reconstruction algorithm \citep{Six2019,Frenkel2022,Yang2021,Batenburg2011}, but their application requires prior information and extra work.
Others might eagerly await ``spectral CT'' \citep{Liu2023}, yet technical and economic challenges are still limiting the capability and availability of such machines.
Dual-/Multi-Energy CT might be another route to overcome the limitations of monochrome detector screens, and they might combine well with iterative, model-based reconstruction techniques.
\subsection{Non-Intuitive Inertia}
\label{sec:orgf996d03}
It is perfectly normal that physical measurements are subject to measurement inaccuracy.
However, the error range must be quantified and reported.

Above, I demonstrated an analytical assessment of which error contributions should be expected, as well as a numerical sensitivity analysis.
I also quantified the actual regression uncertainty with probabilistic statistical modeling.
The results enable an evaluation of the relative importance of different parameters for the accuracy of the outcome.
For example, changes in the mass or in the regression outcome directly influence the moment of inertia, at a rate higher than the input error.
A \(\vec{COM}\) miscalculation might cause errors of less magnitude, but those are not necessarily isotropic.


Yet by far the greatest effect was due to a change in the regression type, switching from linear to exponential.
Neither of these is accurate, yet in the absence of a model which covers the nature of attenuation (which would rather have to adjust the reconstruction algorithm than the regression), there is no other reasonable option.
The exponential fit seemed plausible from the BMD phantoms, which are designed to resemble ossified tissue.
The majority of observed femur gray values were not far off the exponential regression line.
Nevertheless, the calculated moment of inertia was about \(30 \%\) off the reference.
Note that this error is the sum of both an over-estimation in the linear reference regression, and an under-estimation of \(I\) in the exponential attempt.
Also, the high percentage difference is explained by the fact that moment of inertia values are relatively small in baseline magnitude.
A shortcoming of the present study is that I am unable to determine a ``ground truth'' inertia for the test object, lacking sufficiently accurate physical measurement apparatus and expertise.


Nevertheless, the presented findings have practical application.
Any regression should be as accurate as possible (optimal exposure and dynamic range, noise reduction e.g. by more averaging, beam hardening filter, more/diverse calibration objects).
A probabilistic regression is favorable, since it yields error margins for the regression parameters.
With those, a sensitivity analysis can and should be performed.
Results I obtain are technically accurate, confirming that cheap plastic parts are sufficient and might be preferred in most situations over expensive calibration phantoms.


I also evaluated scaling voxel-wise density so that the mass matches the known total mass is not an option: it can amplify issues from beam hardening (Fig. \ref{fig:cupping_results}) or from marker streaks and thus make the outcome more unpredictable.
The rationale behind scaling was that, for moment of inertia, the distribution of densities is relevant.
Scaling masses is identical to adjusting the intercept parameter in the linear regression (Tabs. \ref{tab:regressions}, \ref{tab:sensitivity}).
An alternative, which I did not attempt, would be to adjust the regression slope so to match the mass checksum.
However, both these adjustments (regression intercept/mass scaling, regression slope) have conceptual flaws.
Firstly, the resulting modified regression would not correctly fit any of the calibration objects, and the question arises which of the measurements is ``more trustworthy'' or ``more true''.
Second, slope adjustment over-values the ``air'' observation: ``air'' has a negligible density, yet might have a non-negligible attenuation (which is visible from its gray value distribution; x-ray flux strongly depends on source-detector-distance in non-vacuum).
This (energy-dependent) air issue might actually explain the aforementioned systematic errors in the supplemented regression plots of \citet{Durston2022}.
Third, again, there is no mechanistic reason to assume a linear relation; the polymer plastics of identical chemical composition are a rare corner case, most other series of objects could be non-linear.
\subsection{Inertia Components}
\label{sec:org69309fe}
To my knowledge, this study is the first to perform a detailed sensitivity analysis of the CT density calibration.
Sensitivity is high, so are the measured error margins, which implies that potential inaccuracies are potentially fatal for the derived measurements.

Yet whether or not an error margin is ``fatal'' also depends on the field of application.
The purpose of the inertial estimation is to calculate joint moments in an inverse dynamic workflow.
Joint moments of a limb in ground contact are dominated by the ground reaction force, i.e. by the gravitational load of the animal and by the dynamics of the heavy torso and head (which constitute \(75 \%\) of the mass of the animal, Tab. \ref{tab:pigletdissection}).
For grounded limbs, the inertial properties of the individual limb segments themselves are negligible (order of magnitude of limb segment mass \emph{versus} total mass), although, depending on the behavior under study, accelerations might be high and caution might be appropriate.
When calculating the inertia of limbs in the swing phase, the inertial components dominate the joint moment calculations.
In any of those cases, measurement inaccuracy should receive a close inspection.


There is another aspect to the miscalculation of inertials: the ``rigid body'' assumption.
Segments \emph{in vivo} are not rigid.
Anisotropic muscle contractions and internal segmental mass movements (e.g. radius-ulna rotation) will shift the \(\vec{COM}\) and might even rotate the inertial axes.
This is practically hard to capture for most dynamical behaviors (though it might be modeled with dynamic CAD models), but the magnitude of this problem depends on the study question.

Another factor is the preservation of the specimen (frozen in this case; formol, alcohol, staining, \ldots{}) which has an influence on the chemical composition and inertial properties (not assessed herein).
Finally, in vivo movements are often restricted to one or few axes.
If a femur swings forward during walking, it will mostly rotate around the medio-lateral axis.
In that case, the moment of inertia around that axis is relevant.
The segment might slightly rotate around its long axis, but long axis rotation will hardly affect total momentum due to a low \(I\) on that axis.
Thus, in this case, the (uncertainty of the) medio-lateral inertial component demands special attention.
\subsection{Lack of Alternatives}
\label{sec:org73a4be9}
The femur CT scan used in this experiment was recorded with all necessary diligence, under ideal conditions, with good image quality and resolution.
For example, a beam hardening filter was used, sample mounting was flawless, contrast and dynamic range were optimized.
Still, I see a distribution for gray values which hinders determining the femur mass at an accuracy better than \(10 \%\) (see probabilistic regression).

The proximal conclusion would be to reject the density estimation from CT data.
However, such an attitude demands the evaluation of alternatives.



The mass can be measured accurately with sufficiently fine scales, yet dissection of segments to determine segment-wise mass literally introduces fuzzy edges.
Center of mass can be estimated by pendulum methods, yet in addition to the dissection issues, the problem of accurately relating the \(\vec{COM}\) to an anatomical reference frame arises.
The conventional instrument to measure moment of inertia is the trifilar pendulum \citep{Korr1962,Wells1987,Schedlinski2001}.
As \citet{Durston2022} have demonstrated, this method is far from trivial, requires a well-prepared setup, and still suffers from limitations (e.g. that all axes have to be measured separately, and then transformed to an anatomical reference frame).
Pendulum methods usually rely on negligibility of the instruments' masses and on the small angle approximation.
Hence, the accuracy of these methods remains hard to estimate.


A slightly more involved approach is the utilization of model-based inertials \citep{Hutchinson2007,Allen2009}.
This approach could be termed the ``indirect CT-based'' method: volumetric images are segmented into volumes of homogeneous tissue, and tissue types can be assigned their approximate physical density.
Full CAD models might be assembled, and to a certain extent these might even incorporate dynamic mass redistribution during locomotion.
The method requires good soft tissue contrast of the scans, e.g. by applying staining protocols.
Since tissue is not homogeneous, but continuous transitions can occur (e.g. muscle-tendon), a validation and error estimate of this approach would be required.
The main challenges of this approach are the workload of segmentation, and again transferability to \emph{in vivo} measurements


Finally, an alternatives to CT scans for the present purpose might be \emph{better} CT scans.
Projection images are monochrome, not monochromatic, and the step towards ``spectral CT'' might improve density estimation by incorporating models of chemical composition.
This will doubtlessly require technical and computational innovation such as new algorithms, most likely algebraic ones which incorporate a measured source spectrum, which iteratively adjust a model volume to match simulated and real CT projections (which would then be polychromatic).
All those are promising developments that would certainly impact the accuracy of inverse dynamic models derived from XROMM experiments.



Neither of these options currently yields quick, transferable, generalizable results, which might seem like CT gray value regression should be the method of choice, despite considerable error.
However, conceptual and practical issues from CT gray value conversion uncovered in this study demand caution and careful evaluation of the limited circumstances in which CT scans may yield usable inertial approximations.
Doubts about measurement accuracy must count as an obstacle in the ethical evaluation of planned experiments.
It must be stated that there currently is no validated method to reliable and efficiently measure dynamic inertials.


\clearpage
\section{Summary}
\label{sec:org52c0a6f}
At the end of this chapter, it should be clear that using CT images to calculate inertial properties has a number of conceptual and computational pitfalls which can put serious doubt on the results.
Tomographic images depict attenuation, not physical density, and though these are occasionally correlated, they remain fundamentally different properties of matter.
Above, I discussed potential flaws in the CT-based approximation of mass, center of mass, and mass moment of inertia, as well as corner cases in which the calculation might co-incidentally yield appropriate results.
In all other cases, the calculation will certainly yield wrong results.
The magnitude of the error is non-negligible, since some axes of inertia in balance equations have very small base magnitude and are therefore susceptible to faults.


However, with all that objection put to paper, it must be stated that there is currently no reliable, efficient method to measure inertial properties of biological systems.
There is no ``density tomography'', CT-based regression is error-prone, and common alternative methods potentially suffer from equally large error margins for other reasons (e.g. dissection inaccuracy, non-static configuration during movements, preservation issues).

Using attenuation values to calculate densities might be about as accurate as setting \(\pi \equiv 3\) in physics classes (a totally valid thing to do, if at the same time setting \(g \equiv 10\)).
Typical pitfalls (beam hardening, metal beads, symmetries) are known and can be avoided, keeping track of error magnitudes is essential (by performing a sensitivity analysis, involving e.g. Gaussian error propagation and probabilistic regression).
Under this condition, the calculated inertials might be a just sufficiently accurate approximation of the true values for some experimental questions; yet accuracy must be validated and cannot be taken for granted.



\clearpage
\section{Acknowledgements}
\label{sec:orgbc7376c}
A significant part of this project was supported by the following fabulous people.
A lot of the personal insights about x-ray theory herein have been made possible by my time at the FleXCT (2022-2023). Thanks to Jan and Jan for this opportunity, and special thanks to the colleagues at the VisionLab, especially Joaquim, for the great discussions!
Jonathan Brecko (``naturalscience'' museum Brussels) and Björn De Samber (FleXCT) were the operators performing the CT scans.
Thank you to Chris Van Ginneken for the most accurate piglet dissection possible (which I was allowed to assist).
Jan Scholliers designed and printed the most ingenious radio-translucent clamp-on-a-motor construction possible, thank you also for the great support on all the parts of this project!
Maja Mielke, Joaquim Sanctorum, Peter Aerts and Chris Van Ginneken provided helpful comments on an earlier version of this manuscript.
