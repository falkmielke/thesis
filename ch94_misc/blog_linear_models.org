#+title: Blog Linear Models
#+subtitle: Insights on the complexity of simple models.


* Problem
Last year, I [[https://doi.org/10.3389/fvets.2023.1111140][published]] one of the major findings of my PhD project.
It involved a more or less complicated, but linear model with a reversed modeling approach which relied on predictive sampling.


The manuscript passed the reviewers' critical assessment, but an adapted version has now faced another jury evaluation as a chapter of [[https://doi.org/10.31237/osf.io/dejkg][my PhD thesis]].
One big question arose (which I am honestly happy about): the model is trained on data from one group of piglets, and then applied to a different group.

#+begin_quote
Would [I, the author] expect to be able to predict something the model is not trained for?
#+end_quote

In other words: does predictive sampling have a bias towards the training outcome?
(This might read a bit cryptic, but I will get to the background below.)


This concern is valid, fundamental, and I was not immediately able to give a conclusive answer.
Working on an appropriate response in revision of my thesis led me back to the inner workings of the linear models which I implemented.
Below, I will summarize some of my considerations, which are too basic, or too general, and therefore too extensive for the thesis chapter.

* Abstract / TL;DR
Whether or not out-of-sample prediction can elucidate group differences depends on multiple input factors.
+ difference of the training- and test distribution
+ slope magnitude (steepness)
+ model complexity/size, i.e. number of slopes
+ residual variation / noise

These became apparant by creating a simulation environment which emulates various model structures in the statistics toolbox I used.
Bivariate distribution plots are particularly valuable for analyzing the outcome.


* Introduction & Methods
** Linear Models Basics
Acknowledged, this might be too basic; yet I find it good practice to clarify my terminology.


I use a probabilistic framework to implement my models, namely `PyMC` (version 5.10.2, https://docs.pymc.io).
Regardless of framework, any linear model has a general structure.
It tries to capture the relation of an *outcome variable* \(y\) to one or multiple *input variables* \(x_i\).
Because the relationship is approximately linear, i.e. proportional, i.e. \(y \sim x\), the model also contains a *slope* \(b_i\) and an *intercept* \(a\).
Finally, there is a model residual, \(\epsilon\), which is needed to ensure that the "equals" sign holds for each observation in the following (vectorial) model equation:

\[y = a + b \cdot x + \epsilon\]

Which is a compact way to write:
\[\begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix} = a + b \cdot \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} + \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{pmatrix}\]


The variables \(x\) and \(y\) are our observed, putatively related data columns ("observables").
The choice of which is "input" and which "outcome" is arbitrary, but usually logically constrained.
We tend to call \(x\) on the RHS a "predictor", because it is the variable which predicts \(y\).
There are a total of \(n\) observations.
Parameters \(a\) and \(b\) are the model parameters, to be adjusted in the modeling procedure (e.g. by regression optimization, or in my case MCMC sampling).
The residual \(\epsilon\) is the difference between the (modeled) \(a+b\cdot x\) and the actual \(y\).
This problem is effectively two-dimensional: one dimension is \(x\), the other \(y\); all the other numbers are a fixed description of their relation.


One additional level of complexity: we may use multiple predictors and slopes.
\[y = a + b_1 \cdot x_1 + b_2 \cdot x_2 + \ldots + b_i \cdot x_i + \epsilon = a + \sum\limits_i b_i \cdot x_i + \epsilon\]

This problem is effectively multi-dimensional (\(i+1\)-dimensional); the \(y\) is chosen to be special by describing it as a function of the other observables.


Predictive modeling, in a nutshell, uses parameter distributions instead of "single numbers" for model parameters.
Take the intercept, for example: it must be a number, we might never know what it exactly is, but we can estimate how likely the "true" intercept will have been a certain number (given the observed data).
Technically, the trick is elegant: we just add another (hidden, tensor) dimension, and let the computer /try/ a multitude of values to see which give better outcomes (MCMC sampling).
The algorithm gradually narrows an initially wide, "uninformed" distribution to something that makes the model fit the data quite well.


** (Limited) Visualization: Bivariate Distribution Plots
A great way to illustrate the linear model is a [[https://seaborn.pydata.org/tutorial/distributions.html#plotting-joint-and-marginal-distributions][bivariate distribution plot]].
They visualize the two observables (or: two of the many observables), both their relation as the scatter plot, and the distribution of values on the margins.


Here an example:
#+attr_html: :width 80%
[[./show/bivariate_distribution.png]]

You see some fake data: one "predictor" (\(x\)) shown on the horizontal axis, and the "outcome" variable on the vertical axis.
Gray scatter dots are the observations.
Note the non-equal axis scaling.
The black line is the (conventional) regression model for the model of the formula above.
Distributions are depicted as histograms on the margins.


You can think of this linear model in an "input-output" way: data drops in from within the range of the horizontal axis, and is projected to the vertical axis by the black line.
Think of raindrops, falling down from the gray histogram atop, and being diverted by exactly \(\frac{\pi}{2}\) at the exact point where they reach the black line.


The code for this particular plot is as follows, with the toolbox [[http://mielke-bio.info/falk/code/LinearModelSimulation.py?download&target=_blank][available here]].

#+begin_src python :export both :results none :session regression

import LinearModelSimulation as LMS
import scipy.stats as STATS
import numpy as NP

sim = LMS.Simulation( slopes = [0.45] \
                    , intercept = 0.2 \
                    , x_range = [-0.25, 0.25] \
                    , n_observations = 2**8 \
                    , noise = 0.1
                    )
# sim.PredictiveSampling()

fig, ax_dict = LMS.MakePlot()

LMS.PlotData(ax_dict, sim, color = '0.5', label = None, zorder = 0)

regression = sim.LinearRegression()
ax = ax_dict['ax']
reg_x = NP.array(sim.settings['x_range'])
reg_y = regression.intercept + regression.slope * reg_x
ax.plot(reg_x, reg_y, 'k-', label = f'regression: y = {regression.intercept:.2f} + {regression.slope:.2f} x')

ax.set_xlabel("x"); ax.set_ylabel("y")
fig.savefig(f"""./show/bivariate_distribution.png""", dpi = LMS.dpi)
LMS.PLT.close()

#+end_src


And here is a nice *plot twist:*
one can easily port the "rain" metaphor to illustrate *probabilistic modeling!*


#+attr_html: :width 80%
[[./show/bivariate_distribution_probabilistic.png]]


Just as with the regular model, data pours in from above, yet it does not deflect upon a fixed line.
Instead, the regression line is variable, summarizing all plausible (given the data) slope-intercept pairs with their respective likelihood.
Think of a rain roof that swings and wiggles over time, projecting the dots to slightly different places.


#+begin_src python :export both :results none :session regression
# additional code for the probabilistic plot
sim.FitModel()
fig, ax_dict = LMS.MakePlot()
ax = ax_dict['ax']
LMS.PlotData(ax_dict, sim, color = '0.5', label = None, zorder = 0)


for chain in sim.trace.posterior.chain:
    for draw in NP.random.choice(sim.trace.posterior.isel(chain = chain).draw.values, 5):
        slope = sim.trace.posterior.isel(chain = chain, draw = draw).slopes.values.ravel()
        intercept = sim.trace.posterior.isel(chain = chain, draw = draw).intercept.values.ravel()
        # certainly there's a better way to work with xarrays.

        reg_y = intercept + slope * reg_x
        ax.plot(reg_x, reg_y, 'k-' \
                , label = f'regression: y = {regression.intercept:.2f} + {regression.slope:.2f} x' \
                , alpha = 0.1)

ax.set_xlabel("x"); ax.set_ylabel("y")
fig.savefig(f"""./show/bivariate_distribution_probabilistic.png""", dpi = LMS.dpi)
LMS.PLT.close()

#+end_src

But that was a digression.
The main takeaway is that *bivariate plots are useful.*
Their only downside is that they are 2D: we can only look at one slope at a time.


** MCMC Sampling and Predictive Sampling
*MCMC Sampling* in the context of probabilistic statistics is the process of adjusting model parameters to achieve the best match between the model outcome and the actual data.
Some call it "regression", some call it "fitting", some call it "training".
It is a sampling procedure because the "sampler", an iteratively adjusting pointer in the model parameter space, runs semi-randomly through that space to see which values are good, and which not.
"Semi-random" is my word for describing that sampling is not fully random (which would be error-prone and time consuming), but that clever update algorithms determine the trace of the sampler.
After an exaggerated lot of iterations, the sampler has hopefully converged to something that accurately depicts the true distribution of parameter values, as good as we can estimate it with the observed data.
We call this the "posterior distribution".


*Predictive sampling* essentially takes all the possible probabilistic samples (pairs of slope and intercept in a simple linear model), also takes all the input data (observed \(x\)), and returns the hypothetical outcome \(y\) for each combination of observation and slope sample.
Because there are so many sample-observation-combinations, the outcome also takes the form of a parameter distribution.


When writing this, I figured that all the above is dramatically simpler in the non-probabilistic case.
One could just multiply all observations with the regression slope.
The reason people don't do it is probably twofold.
First, I guess most conventional linear regression tools lack the convenience functions.
Yet I admit I haven't checked too thoroughly.
Second, the frequentist solution is just a single outcome; yet we know that there is an uncertainty or variation to our modeling result.
To me, it was never directly obvious how to include the residual variation in the (single) prediction.
I am honestly exposing some agnosticism here to hundreds of years of honorable frequentist statistics; the Bayesian route was always more intuitive to me.


Whereas model regression (the actual MCMC sampling) is already finished and posterior distributions are fixed at the stage of "predictive sampling", one can still change the /observations/.
The default option is to use exactly the data the model was trained on, *"in-sample prediction"*.
Instead, one could test any artificial values (*"out-of-sample prediction"*), for example extreme observations or just a random subset.
And yet another common practice is to leave out some data when training, so that it also stays out of the training set.



** Data Segmentation: Training, Validation, Test
And this latter one is the option I chose, for logical reasons.
I segmented data into a training set and a randomly sampled *validation set*, both from the main "control group".
The validation set was left out for model fitting/training.
The third, test set, was the "study group", in my case low birth weight piglets.

# TODO plot of out-of-sample predictive sampling


* Results

** difference of the training- and test distribution
In other words, if the training and test data are (probabilistically) identical, the prediction must match the input.
This is the case, for example, with the validation set: it is a random subset of the training data, therefore identical in data range and distribution.
(Validation is useful nonetheless, because we compare the predicted outcome of each individual data point to the actual observation.)

** slope magnitude
Near-zero slopes make the model indifferent to differences; the model will turn blind to differences on a zero-slope dimension.
Zero-slopes can never cause predictive deviations, even if the test distribution is totally different from the data it was trained on.

** residual variation / noise

** number of slopes

* Discussion
# how each of the effects influence my interpretation
# might be integrated with results
